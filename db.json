{"meta":{"version":1,"warehouse":"4.0.2"},"models":{"Asset":[{"_id":"themes/cactus/source/js/main.js","path":"js/main.js","modified":0,"renderable":1},{"_id":"themes/cactus/source/js/search.js","path":"js/search.js","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/clipboard/clipboard.min.js","path":"lib/clipboard/clipboard.min.js","modified":0,"renderable":1},{"_id":"themes/cactus/source/images/apple-touch-icon.png","path":"images/apple-touch-icon.png","modified":0,"renderable":1},{"_id":"themes/cactus/source/images/favicon-192x192.png","path":"images/favicon-192x192.png","modified":0,"renderable":1},{"_id":"themes/cactus/source/images/favicon.ico","path":"images/favicon.ico","modified":0,"renderable":1},{"_id":"themes/cactus/source/images/logo.png","path":"images/logo.png","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/jquery/jquery.min.js","path":"lib/jquery/jquery.min.js","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Black.eot","path":"lib/vazir-font/Vazir-Black.eot","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Black.ttf","path":"lib/vazir-font/Vazir-Black.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Black.woff","path":"lib/vazir-font/Vazir-Black.woff","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Bold.eot","path":"lib/vazir-font/Vazir-Bold.eot","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Bold.ttf","path":"lib/vazir-font/Vazir-Bold.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Black.woff2","path":"lib/vazir-font/Vazir-Black.woff2","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Bold.woff","path":"lib/vazir-font/Vazir-Bold.woff","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Bold.woff2","path":"lib/vazir-font/Vazir-Bold.woff2","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Light.woff","path":"lib/vazir-font/Vazir-Light.woff","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Light.ttf","path":"lib/vazir-font/Vazir-Light.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Light.eot","path":"lib/vazir-font/Vazir-Light.eot","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Light.woff2","path":"lib/vazir-font/Vazir-Light.woff2","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Medium.ttf","path":"lib/vazir-font/Vazir-Medium.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Medium.woff","path":"lib/vazir-font/Vazir-Medium.woff","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Medium.eot","path":"lib/vazir-font/Vazir-Medium.eot","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Medium.woff2","path":"lib/vazir-font/Vazir-Medium.woff2","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Regular.eot","path":"lib/vazir-font/Vazir-Regular.eot","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Regular.ttf","path":"lib/vazir-font/Vazir-Regular.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Regular.woff","path":"lib/vazir-font/Vazir-Regular.woff","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Regular.woff2","path":"lib/vazir-font/Vazir-Regular.woff2","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Thin.eot","path":"lib/vazir-font/Vazir-Thin.eot","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Thin.ttf","path":"lib/vazir-font/Vazir-Thin.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Thin.woff","path":"lib/vazir-font/Vazir-Thin.woff","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Thin.woff2","path":"lib/vazir-font/Vazir-Thin.woff2","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Variable.eot","path":"lib/vazir-font/Vazir-Variable.eot","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Variable.ttf","path":"lib/vazir-font/Vazir-Variable.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Variable.woff","path":"lib/vazir-font/Vazir-Variable.woff","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Variable.woff2","path":"lib/vazir-font/Vazir-Variable.woff2","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/vazir-font/font-face.css","path":"lib/vazir-font/font-face.css","modified":0,"renderable":1},{"_id":"themes/cactus/source/css/rtl.styl","path":"css/rtl.styl","modified":0,"renderable":1},{"_id":"themes/cactus/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/justified-gallery/css/justifiedGallery.min.css","path":"lib/justified-gallery/css/justifiedGallery.min.css","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/justified-gallery/js/jquery.justifiedGallery.min.js","path":"lib/justified-gallery/js/jquery.justifiedGallery.min.js","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGL-BoldItalic.ttf","path":"lib/meslo-LG/MesloLGL-BoldItalic.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGL-Italic.ttf","path":"lib/meslo-LG/MesloLGL-Italic.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGM-Bold.ttf","path":"lib/meslo-LG/MesloLGM-Bold.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGM-BoldItalic.ttf","path":"lib/meslo-LG/MesloLGM-BoldItalic.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGL-Regular.ttf","path":"lib/meslo-LG/MesloLGL-Regular.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGL-Bold.ttf","path":"lib/meslo-LG/MesloLGL-Bold.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGM-Regular.ttf","path":"lib/meslo-LG/MesloLGM-Regular.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGM-Italic.ttf","path":"lib/meslo-LG/MesloLGM-Italic.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGS-Bold.ttf","path":"lib/meslo-LG/MesloLGS-Bold.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGS-BoldItalic.ttf","path":"lib/meslo-LG/MesloLGS-BoldItalic.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGS-Italic.ttf","path":"lib/meslo-LG/MesloLGS-Italic.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGS-Regular.ttf","path":"lib/meslo-LG/MesloLGS-Regular.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/css/all.min.css","path":"lib/font-awesome/css/all.min.css","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-brands-400.ttf","path":"lib/font-awesome/webfonts/fa-brands-400.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-brands-400.woff2","path":"lib/font-awesome/webfonts/fa-brands-400.woff2","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-regular-400.ttf","path":"lib/font-awesome/webfonts/fa-regular-400.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-regular-400.woff2","path":"lib/font-awesome/webfonts/fa-regular-400.woff2","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-solid-900.ttf","path":"lib/font-awesome/webfonts/fa-solid-900.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-solid-900.woff2","path":"lib/font-awesome/webfonts/fa-solid-900.woff2","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-v4compatibility.ttf","path":"lib/font-awesome/webfonts/fa-v4compatibility.ttf","modified":0,"renderable":1},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-v4compatibility.woff2","path":"lib/font-awesome/webfonts/fa-v4compatibility.woff2","modified":0,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"0bf253577828a937bcc9ee24ebd92c8e8a623f03","modified":1667613245109},{"_id":"source/_posts/.DS_Store","hash":"ccb593d4c654642bd1a4404507fc3beca33d7148","modified":1667613245109},{"_id":"source/_posts/001-start.md","hash":"b54f5f7b4b085aae7d6aaabc156f19cf578b4989","modified":1667609938916},{"_id":"source/_posts/003-web-scraping.md","hash":"23c08e599ff770b2602062d019002d005e42f6c3","modified":1667666145837},{"_id":"source/categories/index.md","hash":"1b94463022d0ea326425e22bbfa2993449fe9d25","modified":1667666910244},{"_id":"source/_posts/002-seam.md","hash":"2eb891b07503ba61e9fce67cce38cc971a463517","modified":1667666137854},{"_id":"source/tags/index.md","hash":"504e6068e5cf43363725873b08627a1b4b24bf56","modified":1667666905570},{"_id":"source/_posts/002-seam/.DS_Store","hash":"e23dab22415aa0f1350948893df357044a6641a3","modified":1667609938917},{"_id":"source/_posts/002-seam/set1_carved6.jpg","hash":"3a9b5fd338f205b11e41fd08096852015df0c558","modified":1667609939024},{"_id":"source/_posts/002-seam/set1_carved8.jpg","hash":"a0de1fc1f6b7db8ea7a4d8a8c968a3c67917555b","modified":1667609939024},{"_id":"source/_posts/002-seam/set1_original.jpg","hash":"b1a5152088168dfd1b664cf15cfb03baf16c1743","modified":1667609939024},{"_id":"source/_posts/002-seam/set4_carved7.jpg","hash":"8d3641ad37b93469527351d753e60e24061a92eb","modified":1667609939024},{"_id":"source/_posts/002-seam/set4_carved3.jpg","hash":"d163d717d57ce4c1b26e2f5613da8769b3535240","modified":1667609939024},{"_id":"source/_posts/002-seam/set4_original.jpg","hash":"cf63f06601f8dae0f65b2021a5dae355342df38e","modified":1667609939024},{"_id":"themes/cactus/.jshintrc","hash":"2548bd6ce44422edc7e6f9f68061ab47f26c4f57","modified":1667610645363},{"_id":"themes/cactus/LICENSE","hash":"346ece39a983b0e7858c11f785cd846cef9eb875","modified":1667610645363},{"_id":"themes/cactus/.stylintrc","hash":"eb5f48e83657928cb0cbee031373b2cd36ca0083","modified":1667610645363},{"_id":"themes/cactus/README.md","hash":"778ff0b9caf666d2c0dc3413e26ffb862f354173","modified":1667610645363},{"_id":"themes/cactus/gulpfile.js","hash":"e60630581a5ce8ec2100e7d6d50db71aef654c39","modified":1667610645364},{"_id":"themes/cactus/_config.yml","hash":"5342472ed195adf97abb1139a82f7acb07f95aee","modified":1667666998462},{"_id":"themes/cactus/package.json","hash":"9d9dfe0e611e69c0db7a7de193a03a253263d504","modified":1667610645368},{"_id":"themes/cactus/.gitignore","hash":"72267ee409a324fc197c150b3c4bf28b87b709a8","modified":1667610645363},{"_id":"themes/cactus/languages/ar.yml","hash":"81a88b0593fc89de3118d686681b1f69883c847b","modified":1667610645364},{"_id":"themes/cactus/languages/de.yml","hash":"43b2f4e078b042aaae0377a4235216a51ed82e0d","modified":1667610645364},{"_id":"themes/cactus/languages/ca.yml","hash":"b79dd2c21dc6697c635e92db1f661a4b8d5d2305","modified":1667610645364},{"_id":"themes/cactus/languages/en.yml","hash":"6a84970bf69c3e9490e5382747ca2b4c4b4dccde","modified":1667610645364},{"_id":"themes/cactus/languages/es.yml","hash":"2b1fc8b0d636123e9ee39017fa20053bd1913a5a","modified":1667610645364},{"_id":"themes/cactus/languages/default.yml","hash":"6a84970bf69c3e9490e5382747ca2b4c4b4dccde","modified":1667610645364},{"_id":"themes/cactus/languages/fa.yml","hash":"63f32e50953af1c4bd0308a4fca5862b5287c2cb","modified":1667610645364},{"_id":"themes/cactus/languages/fr.yml","hash":"5c07406998f19d219a5a7b65c0d88b6b023f85b2","modified":1667610645364},{"_id":"themes/cactus/languages/nl.yml","hash":"ac0573352ad2c737a7686bcca498b985e7bd6447","modified":1667610645365},{"_id":"themes/cactus/languages/kr.yml","hash":"651fb83991c91b13b53ed55740e5402cf0f1c5e8","modified":1667610645365},{"_id":"themes/cactus/languages/ru.yml","hash":"81b57fcd1977ef534f4bf303dbc1b4710cc7f057","modified":1667610645365},{"_id":"themes/cactus/languages/it.yml","hash":"62800bcae1f2d2454f87f4bcf4d7593848424f61","modified":1667610645365},{"_id":"themes/cactus/languages/pl.yml","hash":"8a2d6dc874d86c38d42c2c861c39590647b5d536","modified":1667610645365},{"_id":"themes/cactus/languages/pt-br.yml","hash":"4859aba788a050c2d5d0b997693b0c8c24b349f7","modified":1667610645365},{"_id":"themes/cactus/languages/tr.yml","hash":"2702914007e6bade9d6861078c0e179ac05bf48c","modified":1667610645365},{"_id":"themes/cactus/layout/404.ejs","hash":"b911da998c160cceb8cd7c4dae709a1374ed2491","modified":1667610645365},{"_id":"themes/cactus/languages/vi.yml","hash":"f84893c3ec3e45875c90069e14b17ed3016ed973","modified":1667610645365},{"_id":"themes/cactus/languages/zh-TW.yml","hash":"2f4e050c9b35a67f4a7278cec3a949533c2ac16a","modified":1667610645365},{"_id":"themes/cactus/languages/zh-CN.yml","hash":"d016060817311addb4c528de440126b975038c31","modified":1667610645365},{"_id":"themes/cactus/layout/index.ejs","hash":"2010ee127d611139ba08c5a933e222c2e58e7a10","modified":1667663168092},{"_id":"themes/cactus/layout/archive.ejs","hash":"5a23d506dd65f9b5fd1d44a73d5e04c935a899e2","modified":1667610645367},{"_id":"themes/cactus/layout/layout.ejs","hash":"8504004f2ed78914f806c6699d9bd722318cbe56","modified":1667610645368},{"_id":"themes/cactus/layout/page.ejs","hash":"c5465d5315a7544aa466b01fd8cfb62917a8bb1d","modified":1667610645368},{"_id":"themes/cactus/scripts/cdn.js","hash":"887edec364d51efa7c524446483188c6ad05adaf","modified":1667610645368},{"_id":"themes/cactus/layout/post.ejs","hash":"f9149f294e6142437c58784c41f1d082a61c8b82","modified":1667610645368},{"_id":"themes/cactus/scripts/merge-configs.js","hash":"2048c3415d96b17b9d84aa44bc0c25f1210525f8","modified":1667610645368},{"_id":"themes/cactus/scripts/errror_404.js","hash":"f83b290e47cb78a2754152fccc34e571a72087bd","modified":1667610645368},{"_id":"themes/cactus/scripts/meta.js","hash":"654868666b6573b2cee7e750b47ad8a3c2ee13a0","modified":1667610645368},{"_id":"themes/cactus/scripts/page_title.js","hash":"fa662dbdb82779af1b95e35ed7ccdf4866a53dee","modified":1667610645368},{"_id":"themes/cactus/scripts/thumbnail.js","hash":"df8829fd8c3119650037eba5ec11bdce06acff9d","modified":1667610645368},{"_id":"themes/cactus/layout/_partial/head.ejs","hash":"95526bec071998144ee0b0fc33f39bb74e5e9c4f","modified":1667610645366},{"_id":"themes/cactus/layout/_partial/header.ejs","hash":"0e06ee826de1af22a63626456ceb8f2b6c0d1555","modified":1667610645366},{"_id":"themes/cactus/layout/_partial/footer.ejs","hash":"12fd63b51472c9c5b8b7d167eb1a96bf1d686c20","modified":1667610645366},{"_id":"themes/cactus/layout/_partial/scripts.ejs","hash":"a901e3c89e4cd1d20a87bfc683b64b6818275946","modified":1667610645367},{"_id":"themes/cactus/layout/_partial/pagination.ejs","hash":"23bf862b3b8a3cd831850504d9b5a24d21b005e7","modified":1667610645366},{"_id":"themes/cactus/layout/_partial/search.ejs","hash":"8b4bf9cf5db0ce762a31fc3baae0f2fc004bece4","modified":1667610645367},{"_id":"themes/cactus/layout/_partial/styles.ejs","hash":"c6bc7e8a422c5bb57f88fed1d1b0694d03e24e74","modified":1667610645367},{"_id":"themes/cactus/source/css/_fonts.styl","hash":"354809b5a64e8a47a66c66fd1a28ac597c1460a6","modified":1667610645369},{"_id":"themes/cactus/layout/_partial/comments.ejs","hash":"4e75035a427fd137ae7f12940209e8e97845df3b","modified":1667610645366},{"_id":"themes/cactus/source/css/_extend.styl","hash":"b6a4e5905a7515dda66919167531a5ab2b3d1fe2","modified":1667610645369},{"_id":"themes/cactus/source/css/_mixins.styl","hash":"1a9e309523df9685e8d088dcff0a809c58e2c392","modified":1667610645377},{"_id":"themes/cactus/source/css/_util.styl","hash":"2bfeb2e2605dd5235693b00c71a212646d2e0410","modified":1667610645379},{"_id":"themes/cactus/source/css/_variables.styl","hash":"69d9c5e95edcaee5ccd8218262b989ce721cce79","modified":1667610645379},{"_id":"themes/cactus/source/css/style.styl","hash":"5d8afa50dd27d083e09d3b09106f98de46e3c7d0","modified":1667610645379},{"_id":"themes/cactus/source/css/rtl.styl","hash":"ff8700e1626feeb53d905a2df2777bda7d1eca50","modified":1667610645379},{"_id":"themes/cactus/source/images/apple-touch-icon.png","hash":"57e2def34682655f41a0be2d083f16765ba7858b","modified":1667610645379},{"_id":"themes/cactus/source/images/favicon-192x192.png","hash":"96e6fcbbb13a5914a6131391e210eb7dfd13d692","modified":1667610645379},{"_id":"themes/cactus/source/images/favicon.ico","hash":"189f9842bcb79a6f8f9e8445bc8bbd773443826b","modified":1667610645380},{"_id":"themes/cactus/source/js/search.js","hash":"914a2ce72fb325106c61600200be823b72bfb39f","modified":1667610645381},{"_id":"themes/cactus/source/js/main.js","hash":"619ac6529d140711e3b14f739a192bb31c4824ff","modified":1667610645381},{"_id":"themes/cactus/layout/_partial/post/actions_desktop.ejs","hash":"aa6218d8d5af1e26e7a0d805b1ea864eca2b88c5","modified":1667610645366},{"_id":"themes/cactus/layout/_partial/post/actions_mobile.ejs","hash":"79b234ff3c264e66b2e71c819228e62bf92b48e4","modified":1667610645366},{"_id":"themes/cactus/layout/_partial/post/gallery.ejs","hash":"9aecd8908e8a684f33dc20c02497c0f1774137c7","modified":1667610645367},{"_id":"themes/cactus/layout/_partial/post/date.ejs","hash":"6f2d1aa9562df343b797d25705f1945323c465fb","modified":1667610645366},{"_id":"themes/cactus/layout/_partial/post/tag.ejs","hash":"e08fae30da060f49c087f6c121868b08eb55c795","modified":1667610645367},{"_id":"themes/cactus/layout/_partial/post/title.ejs","hash":"a060f1c6e3718494a6b1d0e1981ea0bf4e549828","modified":1667610645367},{"_id":"themes/cactus/layout/_partial/post/share.ejs","hash":"1a294382bd14d979525b8ed934d807bc7d083e4d","modified":1667610645367},{"_id":"themes/cactus/layout/_partial/post/category.ejs","hash":"b5bfa049f17868fb09d9d2a7e1d5279fa0381d37","modified":1667610645366},{"_id":"themes/cactus/source/css/_colors/dark.styl","hash":"9aa43b1f23d5d268dfa36bd942d6ce97b7677c4d","modified":1667610645369},{"_id":"themes/cactus/source/css/_colors/light.styl","hash":"d14ef1aa02d0895b6f9321ebfc23a1ec84b054b8","modified":1667610645369},{"_id":"themes/cactus/source/css/_colors/white.styl","hash":"0fcf3bf123a0f904bb2ee04ff33e41c501f05a7b","modified":1667662506477},{"_id":"themes/cactus/source/css/_highlight/androidstudio.styl","hash":"2af0861725f97f0ee2ded67c3d2d4548c62b2d16","modified":1667610645370},{"_id":"themes/cactus/source/css/_colors/classic.styl","hash":"bc09f8777a6c99030da953dfdb84f793c5e4fd85","modified":1667610645369},{"_id":"themes/cactus/source/css/_highlight/arduino-light.styl","hash":"15e8572585cd708221c513dea4bdd89d8fe56c10","modified":1667610645370},{"_id":"themes/cactus/source/css/_highlight/agate.styl","hash":"53027913ed8d4f75ac3e49e76aad824f0df62da3","modified":1667610645369},{"_id":"themes/cactus/source/css/_highlight/ascetic.styl","hash":"32cff3bef6fac3760fe78f203096477052a90552","modified":1667610645370},{"_id":"themes/cactus/source/css/_highlight/arta.styl","hash":"b3e81e3e694ceb8deed178adb8b91013c5120e30","modified":1667610645370},{"_id":"themes/cactus/source/css/_highlight/atelier-cave-dark.styl","hash":"ce63dd8548688d88254405eedfa75b1d7c82449e","modified":1667610645370},{"_id":"themes/cactus/source/css/_highlight/atelier-dune-dark.styl","hash":"c196ff0ee064af0e507823694ae39020addfc280","modified":1667610645370},{"_id":"themes/cactus/source/css/_highlight/atelier-cave-light.styl","hash":"a5be0744a7ecf4a08f600ade4cfd555afc67bc15","modified":1667610645370},{"_id":"themes/cactus/source/css/_highlight/atelier-dune-light.styl","hash":"931435fbc6f974e8ce9e32722680035d248a9dc1","modified":1667610645371},{"_id":"themes/cactus/source/css/_highlight/atelier-estuary-dark.styl","hash":"0bb16a4eff93688f40787abc2f9e56e7d5cc93e7","modified":1667610645371},{"_id":"themes/cactus/source/css/_highlight/atelier-estuary-light.styl","hash":"344276ca9b27e51d4c907f76afe5d13cf8e60bdf","modified":1667610645371},{"_id":"themes/cactus/source/css/_highlight/atelier-forest-dark.styl","hash":"effbc5d75fa87203c847039869c22031b40d5b7d","modified":1667610645371},{"_id":"themes/cactus/source/css/_highlight/atelier-forest-light.styl","hash":"95228d9f2102fad425536aac44b80b2cba1f5950","modified":1667610645371},{"_id":"themes/cactus/source/css/_highlight/atelier-heath-dark.styl","hash":"9a2e9a1d0a01bbdf158560c3ed1c134e098b2c68","modified":1667610645371},{"_id":"themes/cactus/source/css/_highlight/atelier-lakeside-dark.styl","hash":"10ee3882fca7b97a37bd309d2d35fce9868647bb","modified":1667610645371},{"_id":"themes/cactus/source/css/_highlight/atelier-heath-light.styl","hash":"8c8c2e445abef85273be966d59770e9ced6aac21","modified":1667610645371},{"_id":"themes/cactus/source/css/_highlight/atelier-plateau-dark.styl","hash":"84c80e6f67f62fce958d25817c277d2360272617","modified":1667610645372},{"_id":"themes/cactus/source/css/_highlight/atelier-lakeside-light.styl","hash":"2c54cb9bdb259ae3b5b29f63ac2469ed34b08578","modified":1667610645371},{"_id":"themes/cactus/source/css/_highlight/atelier-plateau-light.styl","hash":"d1a05fdd1ededc9063d181ab25bad55a164aeb4a","modified":1667610645372},{"_id":"themes/cactus/source/css/_highlight/atelier-savanna-dark.styl","hash":"e32c1c70def8060fce5e790979a126da650ac642","modified":1667610645372},{"_id":"themes/cactus/source/css/_highlight/atelier-seaside-dark.styl","hash":"2edf385215bbe1985b1a10106525d362667d28c2","modified":1667610645372},{"_id":"themes/cactus/source/css/_highlight/atelier-savanna-light.styl","hash":"f8244c93711c7cb59dd79d2df966806b30d171ea","modified":1667610645372},{"_id":"themes/cactus/source/css/_highlight/atelier-seaside-light.styl","hash":"0597342da6e2d0c5bdcc7d42dabb07322b1a4177","modified":1667610645372},{"_id":"themes/cactus/source/css/_highlight/atelier-sulphurpool-dark.styl","hash":"538a14321193cd8abf2ddc484306631e54149ffb","modified":1667610645372},{"_id":"themes/cactus/source/css/_highlight/atelier-sulphurpool-light.styl","hash":"efa52713efc468abeeb2b9299704371583b857de","modified":1667610645372},{"_id":"themes/cactus/source/css/_highlight/brown-paper.styl","hash":"c2326ba20a5020a66ca7895258d18833327d4334","modified":1667610645373},{"_id":"themes/cactus/source/css/_highlight/codepen-embed.styl","hash":"8b7b34484f76a6c2c3b1a9e49abb9b382f439ae8","modified":1667610645373},{"_id":"themes/cactus/source/css/_highlight/color-brewer.styl","hash":"2a439d6214430e2f45dd4939b4dfe1fe1a20aa0f","modified":1667610645373},{"_id":"themes/cactus/source/css/_highlight/brown-papersq.png","hash":"3a1332ede3a75a3d24f60b6ed69035b72da5e182","modified":1667610645373},{"_id":"themes/cactus/source/css/_highlight/far.styl","hash":"aaac3028f5e33123cd123a583cddc9290c45ec8e","modified":1667610645374},{"_id":"themes/cactus/source/css/_highlight/dark.styl","hash":"f5e6e75958de59e87fc6be3a1668e870e20bc836","modified":1667610645373},{"_id":"themes/cactus/source/css/_highlight/googlecode.styl","hash":"bda816beee7b439814b514e6869dc678822be1bc","modified":1667610645374},{"_id":"themes/cactus/source/css/_highlight/darkula.styl","hash":"9717efa9194837ba3fb4d762997d33075dcf8bfa","modified":1667610645373},{"_id":"themes/cactus/source/css/_highlight/docco.styl","hash":"b1c176378bb275f2e8caa759f36294e42d614bf1","modified":1667610645373},{"_id":"themes/cactus/source/css/_highlight/github-gist.styl","hash":"48211a03d33e7f7ada0b261162bea06676155a71","modified":1667610645374},{"_id":"themes/cactus/source/css/_highlight/foundation.styl","hash":"bf8ddc94b4ad995b8b8805b5a4cf95004553fdac","modified":1667610645374},{"_id":"themes/cactus/source/css/_highlight/grayscale.styl","hash":"bf37d8b8d1e602126c51526f0cc28807440228ed","modified":1667610645374},{"_id":"themes/cactus/source/css/_highlight/gruvbox-dark.styl","hash":"76b744c14fd5600bea64731c05df97c2df75523f","modified":1667610645374},{"_id":"themes/cactus/source/css/_highlight/highlightjs.styl","hash":"0e198b7a59191c7a39b641a4ddd22c948edb9358","modified":1667610645374},{"_id":"themes/cactus/source/css/_highlight/github.styl","hash":"3336aeba324c6d34a6fd41fef9b47bc598f7064c","modified":1667610645374},{"_id":"themes/cactus/source/css/_highlight/idea.styl","hash":"a02967cb51c16a34e0ee895d33ded2b823d35b21","modified":1667610645374},{"_id":"themes/cactus/source/css/_highlight/hybrid.styl","hash":"b8eb5c69d12f2ee5ebc50265ae271699d7f1a8d3","modified":1667610645374},{"_id":"themes/cactus/source/css/_highlight/index.styl","hash":"002d5596f6379cc87dbd43d9145bc764aa666be1","modified":1667610645374},{"_id":"themes/cactus/source/css/_highlight/hopscotch.styl","hash":"1378a6bc67a32c0cbff72ab771268b53f9aa586d","modified":1667610645374},{"_id":"themes/cactus/source/css/_highlight/kimbie.dark.styl","hash":"45dbb168f22d739d0109745d2decd66b5f94e786","modified":1667610645375},{"_id":"themes/cactus/source/css/_highlight/ir-black.styl","hash":"53e5d74326a4527b92272bbd6946d4fec92720e8","modified":1667610645375},{"_id":"themes/cactus/source/css/_highlight/kimbie.styl","hash":"51b889ca7c6fe178cfbbe28d875a6ea427184441","modified":1667610645375},{"_id":"themes/cactus/source/css/_highlight/magula.styl","hash":"16d323f989b1420a0f72ef989242ece9bf17a456","modified":1667610645375},{"_id":"themes/cactus/source/css/_highlight/kimbie.light.styl","hash":"61f8baed25be05288c8604d5070afbcd9f183f49","modified":1667610645375},{"_id":"themes/cactus/source/css/_highlight/obsidian.styl","hash":"199e28326be8590883f0813ebbd54fcfaa4750fd","modified":1667610645375},{"_id":"themes/cactus/source/css/_highlight/mono-blue.styl","hash":"4c89a6ae29de67c0700585af82a60607e85df928","modified":1667610645375},{"_id":"themes/cactus/source/css/_highlight/monokai.styl","hash":"f87be027848ea6bee623a08ad1e17b2f5b7937ee","modified":1667610645375},{"_id":"themes/cactus/source/css/_highlight/paraiso.styl","hash":"75f181eece6b71d033ea0c8d6cf00ae7efb9e29b","modified":1667610645376},{"_id":"themes/cactus/source/css/_highlight/paraiso-dark.styl","hash":"f1537bd868579fa018ecdbfd2eb922dcf3ba2cac","modified":1667610645375},{"_id":"themes/cactus/source/css/_highlight/pojoaque.jpg","hash":"c5fe6533b88b21f8d90d3d03954c6b29baa67791","modified":1667610645376},{"_id":"themes/cactus/source/css/_highlight/paraiso-light.styl","hash":"d224d1df0eb3395d9eea1344cee945c228af2911","modified":1667610645376},{"_id":"themes/cactus/source/css/_highlight/rainbow.styl","hash":"c0cf97aae3e10fdcd10414547a711c9effbc39b8","modified":1667610645376},{"_id":"themes/cactus/source/css/_highlight/railscasts.styl","hash":"b6674db9210e0c4444e4835fff2d1361f3ebd64c","modified":1667610645376},{"_id":"themes/cactus/source/css/_highlight/school-book.png","hash":"711ec983c874e093bb89eb77afcbdf6741fa61ee","modified":1667610645376},{"_id":"themes/cactus/source/css/_highlight/monokai-sublime.styl","hash":"c385b11345894be7e6ce3c5f08663e199933b8e4","modified":1667610645375},{"_id":"themes/cactus/source/css/_highlight/pojoaque.styl","hash":"4e7b6b046b8575ac749f6aec4e953a62ada27a36","modified":1667610645376},{"_id":"themes/cactus/source/css/_highlight/solarized-light.styl","hash":"aa0dd3fd25c464183b59c5575c9bee8756b397f2","modified":1667610645377},{"_id":"themes/cactus/source/css/_highlight/school-book.styl","hash":"d43560fe519a931ce6da7d57416d7aa148441b83","modified":1667610645376},{"_id":"themes/cactus/source/css/_highlight/sunburst.styl","hash":"af3eec0fd56151e55bbd49c31b151f36717611d8","modified":1667610645377},{"_id":"themes/cactus/source/css/_highlight/tomorrow-night-bright.styl","hash":"7674fecb6d27350727dc0d2dc93bc018382ebbd0","modified":1667610645377},{"_id":"themes/cactus/source/css/_highlight/tomorrow-night-eighties.styl","hash":"28d751075ebabf7d0327a36f725076fe82fdf626","modified":1667610645377},{"_id":"themes/cactus/source/css/_highlight/tomorrow-night-blue.styl","hash":"f24c17d0ab815dcfaab3438cb9fe2ab4839f5e0d","modified":1667610645377},{"_id":"themes/cactus/source/css/_highlight/solarized-dark.styl","hash":"90c9da5aa594383697e5b18892a7f95beb053f55","modified":1667610645376},{"_id":"themes/cactus/source/css/_highlight/tomorrow.styl","hash":"15779cf6846725c7c35fc56cac39047d7e0aec1c","modified":1667610645377},{"_id":"themes/cactus/source/css/_highlight/tomorrow-night.styl","hash":"16ba09b2db501e4e3e2e7d62595d9bf935bf27c4","modified":1667610645377},{"_id":"themes/cactus/source/css/_highlight/xcode.styl","hash":"5e8532ae8366dcf6a4ef5e4813dc3d42ab3d0a50","modified":1667610645377},{"_id":"themes/cactus/source/css/_highlight/vs.styl","hash":"959a746f4b37aacb5d1d6ff1d57e0c045289d75d","modified":1667610645377},{"_id":"themes/cactus/source/css/_highlight/zenburn.styl","hash":"68ff9332ccc03f9389b15b713415cde016f8088f","modified":1667610645377},{"_id":"themes/cactus/source/css/_partial/archive.styl","hash":"31aef892437d5734a134c34f2a8a6610a8f671c3","modified":1667610645378},{"_id":"themes/cactus/source/css/_partial/article.styl","hash":"258370d8ab98e63804ead9bc030f633ca97a1235","modified":1667610645378},{"_id":"themes/cactus/source/css/_partial/categories.styl","hash":"a43f00e61b3507f130b8a3f8108a4eeca147c2a0","modified":1667610645378},{"_id":"themes/cactus/source/css/_partial/footer.styl","hash":"61c2c7c5f73a0022ec41830bea0812a97f522d7c","modified":1667610645378},{"_id":"themes/cactus/source/css/_partial/comments.styl","hash":"1e90f1fb9d4c155df518cacb5a537e9de9c042c1","modified":1667610645378},{"_id":"themes/cactus/source/css/_partial/header.styl","hash":"048e6afa0b803eb1f344c5239ad00a704bc7ba5f","modified":1667611864601},{"_id":"themes/cactus/source/css/_partial/index.styl","hash":"59c99f4ea3a73bf47ce030df166c5e33d5de31fb","modified":1667610645378},{"_id":"themes/cactus/source/css/_partial/pagination.styl","hash":"950bf517bbe7adb9a9aa4eb5ddec74ffc7598787","modified":1667610645378},{"_id":"themes/cactus/source/css/_partial/search.styl","hash":"159be002780c62a77f46947cf854a7342fba24f4","modified":1667610645379},{"_id":"themes/cactus/source/css/_partial/tags.styl","hash":"d571d5c7c960300d29c5f0ec3fe1140322ecd6b3","modified":1667610645379},{"_id":"themes/cactus/source/lib/clipboard/clipboard.min.js","hash":"6674f81dd01c76be986cf0a8172d1073e56d7ef4","modified":1667610645381},{"_id":"themes/cactus/source/css/_partial/tooltip.styl","hash":"2daff581ec3efaec840cbfdee512195919c32629","modified":1667610645379},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Black.woff2","hash":"7ea4fd7dd4cd4f480af78a0e2c5849eb921b1aeb","modified":1667610645399},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Black.woff","hash":"f6fda2de0348b3e3b7de73267f9f8e97a62f8353","modified":1667610645399},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Bold.woff2","hash":"6e40d0c7669c1adbcbf034bdc459f7bed4d6676d","modified":1667610645401},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Bold.woff","hash":"56e632c9196fac364c66f812a3b4635dd999ad1c","modified":1667610645401},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Light.woff","hash":"1c3dbf17411b1f6a6b22c2b76e9d8511586643d0","modified":1667610645402},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Light.woff2","hash":"50b654d916204c30987d1987abd890ef92085ae3","modified":1667610645402},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Medium.woff","hash":"43a8aaa3fca8721dd32a5d20f7a98dfbc87c97fd","modified":1667610645404},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Medium.woff2","hash":"14b3e257c51a6a11d23b2a078017ff340c9777e4","modified":1667610645404},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Regular.woff2","hash":"a9714ffb842afc74836e64de04b52d8c37c87c8a","modified":1667610645405},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Regular.woff","hash":"235889d59ddad2b1f3243ccaab7733bd713a2a21","modified":1667610645405},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Thin.woff","hash":"c0e784de2eb5261cca244928f8a81fd893c3fe16","modified":1667610645406},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Thin.woff2","hash":"9b03b1a9071709f5b7dbca13412ecef6cb7a2a67","modified":1667610645406},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Variable.woff","hash":"2e8e6d38d361def5f48baac366f04e3db3ed4828","modified":1667610645407},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Variable.woff2","hash":"e213bb26bc7f10e1df3fe2d03d3ecaecd6e6d371","modified":1667610645407},{"_id":"themes/cactus/source/css/_partial/post/actions_desktop.styl","hash":"a1f36f9a3fd5ffcd832bf39e9402678978035d48","modified":1667610645378},{"_id":"themes/cactus/source/css/_partial/post/actions_mobile.styl","hash":"0d2966c1d870392476864af8ee3ba312ba30cb82","modified":1667610645379},{"_id":"themes/cactus/source/lib/vazir-font/font-face.css","hash":"ba0030e1cd28a8caa7a5bb74b98da7c7bb185c90","modified":1667610645407},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-regular-400.ttf","hash":"d1a7eff18db8a47207ea42e34e9d9fbcc66a97a7","modified":1667610645384},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-v4compatibility.ttf","hash":"c77fcea87e0c4953f2b0ac92dc49a31c664b6ef7","modified":1667610645386},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-regular-400.woff2","hash":"be22b700cc80c242da898ef8b7bb96adc4e0899f","modified":1667610645384},{"_id":"themes/cactus/source/lib/justified-gallery/css/justifiedGallery.min.css","hash":"dd3052149d3054f35efb823c68dd78e78aad5875","modified":1667610645387},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-v4compatibility.woff2","hash":"60d794c18c2b58b2b76d2ce17b85c44c48fb2efd","modified":1667610645386},{"_id":"themes/cactus/source/lib/justified-gallery/js/jquery.justifiedGallery.min.js","hash":"ad8f48b4022498078b089fcdd1e8b47faf496931","modified":1667610645387},{"_id":"themes/cactus/source/lib/jquery/jquery.min.js","hash":"b82d238d4e31fdf618bae8ac11a6c812c03dd0d4","modified":1667610645387},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Black.eot","hash":"91152bd73e7ff8d943e3bde3ddb0fa0a018e1c21","modified":1667610645398},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Black.ttf","hash":"b65915e3fa57b5c19995d15dc2341d115c1971b9","modified":1667610645399},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Bold.eot","hash":"5c1c680fade45393e4a5bb4548a092cd5ea6811e","modified":1667610645400},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Bold.ttf","hash":"122bb778b17a152c426a825ee981610a4bd59bf3","modified":1667610645400},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Light.ttf","hash":"df82b80c4d3b11e70dcd269fc62ac97cbfa0414d","modified":1667610645402},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Light.eot","hash":"a059359e9bea17dc2ff2ede955a05bf0dc4d00d0","modified":1667610645401},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Medium.eot","hash":"d9ec1f9f3fefd57e446cbe86dc297f1ff269b6de","modified":1667610645403},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Medium.ttf","hash":"948a091f0fdb8c7ae17d5ef8e51bd8830d65dd9a","modified":1667610645404},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Regular.ttf","hash":"643c28c8f8a2bce1a0d62525aa045cd9883773cd","modified":1667610645405},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Regular.eot","hash":"521c01f0eb79a48025e972ecbe21b0d7fb15437b","modified":1667610645404},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Thin.eot","hash":"a0ea0bdaef00b35544f9a21d25d35db9a79f7189","modified":1667610645405},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Thin.ttf","hash":"6aacb0eecb03c660570b6e159ba5ca97ca7461cf","modified":1667610645406},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Variable.ttf","hash":"1e08b6373c2e086f24776df9b11e4be6bbcc8a4a","modified":1667610645407},{"_id":"themes/cactus/source/lib/vazir-font/Vazir-Variable.eot","hash":"af46f7f4e10a1440a4c97b350622d279143e6798","modified":1667610645407},{"_id":"themes/cactus/source/lib/font-awesome/css/all.min.css","hash":"d3cafed4c6596253c1050ee63897aa0f440e4f65","modified":1667610645382},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-brands-400.woff2","hash":"98564e5517b7b455e80b2cd503e7bb3b52beb930","modified":1667610645383},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-brands-400.ttf","hash":"cfb2c6122bd53141e939ee4ff991a16a29d1bdce","modified":1667610645383},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-solid-900.woff2","hash":"09a731f80844483614ff12f86ccbe41db6736cb5","modified":1667610645386},{"_id":"themes/cactus/source/images/logo.png","hash":"fc1f7ce3aa72436524033c9f1e22d174ccad7fd8","modified":1667609938036},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGL-BoldItalic.ttf","hash":"b7d24ab1e4fad720f31a2b0cca1904ce1740d846","modified":1667610645392},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGM-BoldItalic.ttf","hash":"b542b9591fbf33925d93f0695b6e123a9f0cfd43","modified":1667610645396},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGS-BoldItalic.ttf","hash":"926035f0156cccf1b0ca507347f39bf9c510f51e","modified":1667610645397},{"_id":"themes/cactus/source/lib/font-awesome/webfonts/fa-solid-900.ttf","hash":"97f5404656d9547666479ec64c336467000656ef","modified":1667610645386},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGL-Italic.ttf","hash":"9a23c6898b0943bd3d96c04df9a0f66e919451d8","modified":1667610645394},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGM-Italic.ttf","hash":"93ebc5098cf57a32b7b8d297681f31692c09bdfa","modified":1667610645396},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGS-Italic.ttf","hash":"9d757cc9f928fc83b2133283dd639c12b11d94ad","modified":1667610645397},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGL-Bold.ttf","hash":"34f7db59f1d023294e69976aa20b7d52b86165a4","modified":1667610645391},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGL-Regular.ttf","hash":"6c090d6bff3928fbf8a5f4104e58ed7f421aea7c","modified":1667610645394},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGM-Bold.ttf","hash":"58be4b7760e9a84daa81929d046f9a15c4fd1c1a","modified":1667610645394},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGS-Bold.ttf","hash":"f9918fb93d6ab6850f5d38069a999c311af78816","modified":1667610645397},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGM-Regular.ttf","hash":"20ce1fc7ae1254558ca044ae48283faaa58897e5","modified":1667610645396},{"_id":"themes/cactus/source/lib/meslo-LG/MesloLGS-Regular.ttf","hash":"de559f8d70d5b1ab2810597bfd0b1b9506f3ef01","modified":1667610645397},{"_id":"source/_posts/002-seam/seam-carving-demonstration-video.gif","hash":"34b01e4c3ea4edf8c62d9a5fd7b09caeb9d40ee9","modified":1667609939020},{"_id":"themes/cactus/.DS_Store","hash":"6d607a6bc50ccf068ea73c21412e8eedafbf1d43","modified":1667613184323},{"_id":"themes/cactus/languages/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1667613366054},{"_id":"source/_posts/004-songs.md","hash":"d445868b4b165755712d4f0f17ed45d411d54972","modified":1667674935936},{"_id":"source/about/index.md","hash":"06b9e171815c50ce5dcb0742fb48f66f44a884d9","modified":1667674871692},{"_id":"source/_posts/005-regression.md","hash":"4db57e0742be83510d5b81b466502f81ae96133a","modified":1667676910717},{"_id":"source/_posts/006-missing-data-jags.md","hash":"3af23538bc74d71d49904c26441b1c0ae91ea1aa","modified":1667676977505}],"Category":[{"name":"miscellaneous","_id":"cla38j5c00005mxvg4aa07k35"},{"name":"algorithms","_id":"cla38j5c10007mxvg5cljf9c8"},{"name":"data science","_id":"cla45czwu0000ccvghc13evpd"},{"name":"music","_id":"cla45x13u0000y8vg5bjj9w23"}],"Data":[],"Page":[{"title":"Categories","date":"2022-11-04T23:33:41.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: Categories\ndate: 2022-11-04 19:33:41\ntype: categories\n---\n","updated":"2022-11-05T16:48:30.244Z","path":"categories/index.html","_id":"cla38j5bv0000mxvg1qmc29ht","comments":1,"layout":"page","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"Tags","date":"2022-11-04T23:38:39.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: Tags\ndate: 2022-11-04 19:38:39\ntype: tags\n---\n","updated":"2022-11-05T16:48:25.570Z","path":"tags/index.html","_id":"cla38j5bx0001mxvgg1vj8aup","comments":1,"layout":"page","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"About","date":"2022-11-05T17:45:12.000Z","_content":"\nNothing really. To lazy to add. A reminder to myself: add this section at some point. \n\n","source":"about/index.md","raw":"---\ntitle: About\ndate: 2022-11-05 13:45:12\n---\n\nNothing really. To lazy to add. A reminder to myself: add this section at some point. \n\n","updated":"2022-11-05T19:01:11.692Z","path":"about/index.html","_id":"cla47vdns0000m8vg76v2h6b5","comments":1,"layout":"page","content":"<p>Nothing really. To lazy to add. A reminder to myself: add this section at some point. </p>\n","site":{"data":{}},"excerpt":"","more":"<p>Nothing really. To lazy to add. A reminder to myself: add this section at some point. </p>\n"}],"Post":[{"title":"About This Blog","date":"2022-11-04T22:58:21.000Z","_content":"\n今天是2022年11月4日。\n\n\n","source":"_posts/001-start.md","raw":"---\ntitle: About This Blog\ndate: 2022-11-04 18:58:21\ncategories:\n - [miscellaneous]\n---\n\n今天是2022年11月4日。\n\n\n","slug":"001-start","published":1,"updated":"2022-11-05T00:58:58.916Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cla38j5by0002mxvg346qcj1v","content":"<p>今天是2022年11月4日。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>今天是2022年11月4日。</p>\n"},{"title":"Content-Aware Image Resizing","date":"2021-05-08T04:00:00.000Z","mathjax":true,"_content":"\n### Seam Carving And Insertion\nfinal project for $computational$ $photography$ \n\n#### Aim\nIn this project, I implemented methods to do vertical carving, horizontal carving, vertical insertion, and horizontal insertion in MATLAB. Before deleting or inserting seams, a mask can be manually defined on the input image that specifies an area in the image that is prone to artifacts and thus should be protected from changes.\n\n<!-- more -->\n\n#### Seam Deletion \nHorizontal Carving (removing vertical seams): The function find_seam_vertical uses dynamic programing to retrieve a optimum seam that starts at the top of an image and ends at the bottom. The function keeps two records, one is for the minimum energy at each pixel, the other for the direction of each step. The energy function used in this project is entropy. Using these two records, a matrix same size as the image with 0 indicate not a pixel from the seam and 1 indicating a pixel from the seam can be constructed. The function delete_seam_vertical_mask loops over the image a specified amount of times to remove multiple seams. Vertical Carving (removing horizontal seams) is achieved similarly. \n\n#### Seam Insertion\nHorizontal Insertion (adding vertical seams): The function seam_vertical_record_mask uses the methods to find seams from seam deletion to keep a record of multiple seams that would be removed if a seam deletion is performed. The record is of the index of pixels in the original image that the seams are removing (so if a 5th seam is found, it would not keep track of the new index of the seam from the shrinked image after removing the previous 4 seams, but rather keeping track of positions in the original image). Using the seam indexes, the function seam_vertical_record_mask will duplicate seams that would be deleted in the performed order. Because inserting seams also changes the index of pixels in the original image, a matrix of original pixel index is stored, and the seam inserted is always on the index of the original image pixels. Thus, in the animation of the seam insertion, there will be times that the red line indicating seams became disconnected. This is because previous seams inserted have changed pixel positions in the image, and in order to insert corresponding seams, their pixel positions need to be adjusted too. The duplicated seam is the average of the three pixels surrounding it in the horizontal direction. Vertical Insertion (adding horizontal seams) is achieved similarly. \n\n#### Some Challenges\nWhile implementing seam deletions, the most challenging aspect is to correctly set up dynamic program to find the seam. Because seam pixels need to be connected to previous pixels (can only be off by 1 pixel), when dynamically going through all pixels in an image, the range that the next step pixel can take is restricted to between +1 and -1 of the current pixel position. It also took some tries to figure out that using entropy as the energy function is relatively stable and easy to implement. Other functions that detect edges that I tried did not work as well in seam carving. \nIn my opinion, seam insertion is more challenging than seam carving. The intuitive solution is to find a seam, duplicate it, then find a new seam in the stretched image, and again duplicated it. This repeated seam insertion process, however, would not work because it would just be the same seam being duplicated again and again. Therefore, there is the need find at once set of all seams that would be removed if performing a seam deletion, then reverse-engineering these seams and duplicate each of them. It is also challenging to figure out how to correctly duplicate seams at correct pixels. The seams, if finding them the seam way as in seam deletion, would not work, because the image sizes have changed. Therefore, it is necessary to keep track of seams in the pixel index or positions in the original image’s perspective. And when inserting the duplicated seams, it is necessary again to use the original image’s indexes. \n\nThe other challenging part of the project is to find out a way to minimize artifacts, especially on images where there is a specific object to keep intact. The solution I came up with is to use the getMask function that we have used in multiple other assignments. Using a user-specified black and white mask of the image, it is possible to increase energy on the masked regions when finding seams. Thus, the seam-finding algorithm will seek to avoid these regions.\n\n#### Seam Carving and Insertion Processes\n\n[Code and more](https://github.com/iasnobmatsu/SeamCarvingAndInsertion)\n\n#### Examples\n\noriginal image\n\n{% asset_img set4_original.jpg %}\n\n\nremoving 90 pixels horizontally\n\n{% asset_img set4_carved3.jpg %}\n\ninseting 30 pixels horizontally\n\n{% asset_img set4_carved7.jpg %}\n\n[original image](https://jojo.fandom.com/wiki/Hirohiko_Araki_JoJo_Exhibition_2012?file=Exhib8.jpg)\n\n{% asset_img set1_original.jpg %}\n\nremoving 90 pixels vertically \n\n{% asset_img set1_carved6.jpg %}\n\ninseting 30 pixels vertically\n\n{% asset_img set1_carved8.jpg %}\n\n\n","source":"_posts/002-seam.md","raw":"---\ntitle: Content-Aware Image Resizing\ndate: 2021-05-08\nmathjax: true\ncategories: \n - [data science]\ntags: [computational photography, matlab]\n---\n\n### Seam Carving And Insertion\nfinal project for $computational$ $photography$ \n\n#### Aim\nIn this project, I implemented methods to do vertical carving, horizontal carving, vertical insertion, and horizontal insertion in MATLAB. Before deleting or inserting seams, a mask can be manually defined on the input image that specifies an area in the image that is prone to artifacts and thus should be protected from changes.\n\n<!-- more -->\n\n#### Seam Deletion \nHorizontal Carving (removing vertical seams): The function find_seam_vertical uses dynamic programing to retrieve a optimum seam that starts at the top of an image and ends at the bottom. The function keeps two records, one is for the minimum energy at each pixel, the other for the direction of each step. The energy function used in this project is entropy. Using these two records, a matrix same size as the image with 0 indicate not a pixel from the seam and 1 indicating a pixel from the seam can be constructed. The function delete_seam_vertical_mask loops over the image a specified amount of times to remove multiple seams. Vertical Carving (removing horizontal seams) is achieved similarly. \n\n#### Seam Insertion\nHorizontal Insertion (adding vertical seams): The function seam_vertical_record_mask uses the methods to find seams from seam deletion to keep a record of multiple seams that would be removed if a seam deletion is performed. The record is of the index of pixels in the original image that the seams are removing (so if a 5th seam is found, it would not keep track of the new index of the seam from the shrinked image after removing the previous 4 seams, but rather keeping track of positions in the original image). Using the seam indexes, the function seam_vertical_record_mask will duplicate seams that would be deleted in the performed order. Because inserting seams also changes the index of pixels in the original image, a matrix of original pixel index is stored, and the seam inserted is always on the index of the original image pixels. Thus, in the animation of the seam insertion, there will be times that the red line indicating seams became disconnected. This is because previous seams inserted have changed pixel positions in the image, and in order to insert corresponding seams, their pixel positions need to be adjusted too. The duplicated seam is the average of the three pixels surrounding it in the horizontal direction. Vertical Insertion (adding horizontal seams) is achieved similarly. \n\n#### Some Challenges\nWhile implementing seam deletions, the most challenging aspect is to correctly set up dynamic program to find the seam. Because seam pixels need to be connected to previous pixels (can only be off by 1 pixel), when dynamically going through all pixels in an image, the range that the next step pixel can take is restricted to between +1 and -1 of the current pixel position. It also took some tries to figure out that using entropy as the energy function is relatively stable and easy to implement. Other functions that detect edges that I tried did not work as well in seam carving. \nIn my opinion, seam insertion is more challenging than seam carving. The intuitive solution is to find a seam, duplicate it, then find a new seam in the stretched image, and again duplicated it. This repeated seam insertion process, however, would not work because it would just be the same seam being duplicated again and again. Therefore, there is the need find at once set of all seams that would be removed if performing a seam deletion, then reverse-engineering these seams and duplicate each of them. It is also challenging to figure out how to correctly duplicate seams at correct pixels. The seams, if finding them the seam way as in seam deletion, would not work, because the image sizes have changed. Therefore, it is necessary to keep track of seams in the pixel index or positions in the original image’s perspective. And when inserting the duplicated seams, it is necessary again to use the original image’s indexes. \n\nThe other challenging part of the project is to find out a way to minimize artifacts, especially on images where there is a specific object to keep intact. The solution I came up with is to use the getMask function that we have used in multiple other assignments. Using a user-specified black and white mask of the image, it is possible to increase energy on the masked regions when finding seams. Thus, the seam-finding algorithm will seek to avoid these regions.\n\n#### Seam Carving and Insertion Processes\n\n[Code and more](https://github.com/iasnobmatsu/SeamCarvingAndInsertion)\n\n#### Examples\n\noriginal image\n\n{% asset_img set4_original.jpg %}\n\n\nremoving 90 pixels horizontally\n\n{% asset_img set4_carved3.jpg %}\n\ninseting 30 pixels horizontally\n\n{% asset_img set4_carved7.jpg %}\n\n[original image](https://jojo.fandom.com/wiki/Hirohiko_Araki_JoJo_Exhibition_2012?file=Exhib8.jpg)\n\n{% asset_img set1_original.jpg %}\n\nremoving 90 pixels vertically \n\n{% asset_img set1_carved6.jpg %}\n\ninseting 30 pixels vertically\n\n{% asset_img set1_carved8.jpg %}\n\n\n","slug":"002-seam","published":1,"updated":"2022-11-05T16:35:37.854Z","_id":"cla38j5bz0003mxvgh6ok3n65","comments":1,"layout":"post","photos":[],"link":"","content":"<h3 id=\"Seam-Carving-And-Insertion\"><a href=\"#Seam-Carving-And-Insertion\" class=\"headerlink\" title=\"Seam Carving And Insertion\"></a>Seam Carving And Insertion</h3><p>final project for <mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.439ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"14.432ex\" height=\"2.009ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -694 6379 888\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D450\" d=\"M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(433,0)\"><path data-c=\"1D45C\" d=\"M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(918,0)\"><path data-c=\"1D45A\" d=\"M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1796,0)\"><path data-c=\"1D45D\" d=\"M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(2299,0)\"><path data-c=\"1D462\" d=\"M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(2871,0)\"><path data-c=\"1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(3232,0)\"><path data-c=\"1D44E\" d=\"M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(3761,0)\"><path data-c=\"1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(4122,0)\"><path data-c=\"1D456\" d=\"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(4467,0)\"><path data-c=\"1D45C\" d=\"M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(4952,0)\"><path data-c=\"1D45B\" d=\"M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(5552,0)\"><path data-c=\"1D44E\" d=\"M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(6081,0)\"><path data-c=\"1D459\" d=\"M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z\"></path></g></g></g></svg></mjx-container> <mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.464ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"12.299ex\" height=\"2.034ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -694 5436 899\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D45D\" d=\"M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(503,0)\"><path data-c=\"210E\" d=\"M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1079,0)\"><path data-c=\"1D45C\" d=\"M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1564,0)\"><path data-c=\"1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1925,0)\"><path data-c=\"1D45C\" d=\"M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(2410,0)\"><path data-c=\"1D454\" d=\"M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(2887,0)\"><path data-c=\"1D45F\" d=\"M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(3338,0)\"><path data-c=\"1D44E\" d=\"M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(3867,0)\"><path data-c=\"1D45D\" d=\"M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(4370,0)\"><path data-c=\"210E\" d=\"M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(4946,0)\"><path data-c=\"1D466\" d=\"M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z\"></path></g></g></g></svg></mjx-container> </p>\n<h4 id=\"Aim\"><a href=\"#Aim\" class=\"headerlink\" title=\"Aim\"></a>Aim</h4><p>In this project, I implemented methods to do vertical carving, horizontal carving, vertical insertion, and horizontal insertion in MATLAB. Before deleting or inserting seams, a mask can be manually defined on the input image that specifies an area in the image that is prone to artifacts and thus should be protected from changes.</p>\n<span id=\"more\"></span>\n<h4 id=\"Seam-Deletion\"><a href=\"#Seam-Deletion\" class=\"headerlink\" title=\"Seam Deletion\"></a>Seam Deletion</h4><p>Horizontal Carving (removing vertical seams): The function find_seam_vertical uses dynamic programing to retrieve a optimum seam that starts at the top of an image and ends at the bottom. The function keeps two records, one is for the minimum energy at each pixel, the other for the direction of each step. The energy function used in this project is entropy. Using these two records, a matrix same size as the image with 0 indicate not a pixel from the seam and 1 indicating a pixel from the seam can be constructed. The function delete_seam_vertical_mask loops over the image a specified amount of times to remove multiple seams. Vertical Carving (removing horizontal seams) is achieved similarly. </p>\n<h4 id=\"Seam-Insertion\"><a href=\"#Seam-Insertion\" class=\"headerlink\" title=\"Seam Insertion\"></a>Seam Insertion</h4><p>Horizontal Insertion (adding vertical seams): The function seam_vertical_record_mask uses the methods to find seams from seam deletion to keep a record of multiple seams that would be removed if a seam deletion is performed. The record is of the index of pixels in the original image that the seams are removing (so if a 5th seam is found, it would not keep track of the new index of the seam from the shrinked image after removing the previous 4 seams, but rather keeping track of positions in the original image). Using the seam indexes, the function seam_vertical_record_mask will duplicate seams that would be deleted in the performed order. Because inserting seams also changes the index of pixels in the original image, a matrix of original pixel index is stored, and the seam inserted is always on the index of the original image pixels. Thus, in the animation of the seam insertion, there will be times that the red line indicating seams became disconnected. This is because previous seams inserted have changed pixel positions in the image, and in order to insert corresponding seams, their pixel positions need to be adjusted too. The duplicated seam is the average of the three pixels surrounding it in the horizontal direction. Vertical Insertion (adding horizontal seams) is achieved similarly. </p>\n<h4 id=\"Some-Challenges\"><a href=\"#Some-Challenges\" class=\"headerlink\" title=\"Some Challenges\"></a>Some Challenges</h4><p>While implementing seam deletions, the most challenging aspect is to correctly set up dynamic program to find the seam. Because seam pixels need to be connected to previous pixels (can only be off by 1 pixel), when dynamically going through all pixels in an image, the range that the next step pixel can take is restricted to between +1 and -1 of the current pixel position. It also took some tries to figure out that using entropy as the energy function is relatively stable and easy to implement. Other functions that detect edges that I tried did not work as well in seam carving.<br>In my opinion, seam insertion is more challenging than seam carving. The intuitive solution is to find a seam, duplicate it, then find a new seam in the stretched image, and again duplicated it. This repeated seam insertion process, however, would not work because it would just be the same seam being duplicated again and again. Therefore, there is the need find at once set of all seams that would be removed if performing a seam deletion, then reverse-engineering these seams and duplicate each of them. It is also challenging to figure out how to correctly duplicate seams at correct pixels. The seams, if finding them the seam way as in seam deletion, would not work, because the image sizes have changed. Therefore, it is necessary to keep track of seams in the pixel index or positions in the original image’s perspective. And when inserting the duplicated seams, it is necessary again to use the original image’s indexes. </p>\n<p>The other challenging part of the project is to find out a way to minimize artifacts, especially on images where there is a specific object to keep intact. The solution I came up with is to use the getMask function that we have used in multiple other assignments. Using a user-specified black and white mask of the image, it is possible to increase energy on the masked regions when finding seams. Thus, the seam-finding algorithm will seek to avoid these regions.</p>\n<h4 id=\"Seam-Carving-and-Insertion-Processes\"><a href=\"#Seam-Carving-and-Insertion-Processes\" class=\"headerlink\" title=\"Seam Carving and Insertion Processes\"></a>Seam Carving and Insertion Processes</h4><p><a href=\"https://github.com/iasnobmatsu/SeamCarvingAndInsertion\">Code and more</a></p>\n<h4 id=\"Examples\"><a href=\"#Examples\" class=\"headerlink\" title=\"Examples\"></a>Examples</h4><p>original image</p>\n<img src=\"/2021/05/08/002-seam/set4_original.jpg\" class=\"\">\n<p>removing 90 pixels horizontally</p>\n<img src=\"/2021/05/08/002-seam/set4_carved3.jpg\" class=\"\">\n<p>inseting 30 pixels horizontally</p>\n<img src=\"/2021/05/08/002-seam/set4_carved7.jpg\" class=\"\">\n<p><a href=\"https://jojo.fandom.com/wiki/Hirohiko_Araki_JoJo_Exhibition_2012?file=Exhib8.jpg\">original image</a></p>\n<img src=\"/2021/05/08/002-seam/set1_original.jpg\" class=\"\">\n<p>removing 90 pixels vertically </p>\n<img src=\"/2021/05/08/002-seam/set1_carved6.jpg\" class=\"\">\n<p>inseting 30 pixels vertically</p>\n<img src=\"/2021/05/08/002-seam/set1_carved8.jpg\" class=\"\">\n","site":{"data":{}},"excerpt":"<h3 id=\"Seam-Carving-And-Insertion\"><a href=\"#Seam-Carving-And-Insertion\" class=\"headerlink\" title=\"Seam Carving And Insertion\"></a>Seam Carving And Insertion</h3><p>final project for <mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.439ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"14.432ex\" height=\"2.009ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -694 6379 888\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D450\" d=\"M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(433,0)\"><path data-c=\"1D45C\" d=\"M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(918,0)\"><path data-c=\"1D45A\" d=\"M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1796,0)\"><path data-c=\"1D45D\" d=\"M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(2299,0)\"><path data-c=\"1D462\" d=\"M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(2871,0)\"><path data-c=\"1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(3232,0)\"><path data-c=\"1D44E\" d=\"M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(3761,0)\"><path data-c=\"1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(4122,0)\"><path data-c=\"1D456\" d=\"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(4467,0)\"><path data-c=\"1D45C\" d=\"M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(4952,0)\"><path data-c=\"1D45B\" d=\"M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(5552,0)\"><path data-c=\"1D44E\" d=\"M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(6081,0)\"><path data-c=\"1D459\" d=\"M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z\"></path></g></g></g></svg></mjx-container> <mjx-container class=\"MathJax\" jax=\"SVG\"><svg style=\"vertical-align: -0.464ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"12.299ex\" height=\"2.034ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -694 5436 899\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mi\"><path data-c=\"1D45D\" d=\"M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(503,0)\"><path data-c=\"210E\" d=\"M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1079,0)\"><path data-c=\"1D45C\" d=\"M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1564,0)\"><path data-c=\"1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1925,0)\"><path data-c=\"1D45C\" d=\"M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(2410,0)\"><path data-c=\"1D454\" d=\"M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(2887,0)\"><path data-c=\"1D45F\" d=\"M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(3338,0)\"><path data-c=\"1D44E\" d=\"M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(3867,0)\"><path data-c=\"1D45D\" d=\"M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(4370,0)\"><path data-c=\"210E\" d=\"M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(4946,0)\"><path data-c=\"1D466\" d=\"M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z\"></path></g></g></g></svg></mjx-container> </p>\n<h4 id=\"Aim\"><a href=\"#Aim\" class=\"headerlink\" title=\"Aim\"></a>Aim</h4><p>In this project, I implemented methods to do vertical carving, horizontal carving, vertical insertion, and horizontal insertion in MATLAB. Before deleting or inserting seams, a mask can be manually defined on the input image that specifies an area in the image that is prone to artifacts and thus should be protected from changes.</p>","more":"<h4 id=\"Seam-Deletion\"><a href=\"#Seam-Deletion\" class=\"headerlink\" title=\"Seam Deletion\"></a>Seam Deletion</h4><p>Horizontal Carving (removing vertical seams): The function find_seam_vertical uses dynamic programing to retrieve a optimum seam that starts at the top of an image and ends at the bottom. The function keeps two records, one is for the minimum energy at each pixel, the other for the direction of each step. The energy function used in this project is entropy. Using these two records, a matrix same size as the image with 0 indicate not a pixel from the seam and 1 indicating a pixel from the seam can be constructed. The function delete_seam_vertical_mask loops over the image a specified amount of times to remove multiple seams. Vertical Carving (removing horizontal seams) is achieved similarly. </p>\n<h4 id=\"Seam-Insertion\"><a href=\"#Seam-Insertion\" class=\"headerlink\" title=\"Seam Insertion\"></a>Seam Insertion</h4><p>Horizontal Insertion (adding vertical seams): The function seam_vertical_record_mask uses the methods to find seams from seam deletion to keep a record of multiple seams that would be removed if a seam deletion is performed. The record is of the index of pixels in the original image that the seams are removing (so if a 5th seam is found, it would not keep track of the new index of the seam from the shrinked image after removing the previous 4 seams, but rather keeping track of positions in the original image). Using the seam indexes, the function seam_vertical_record_mask will duplicate seams that would be deleted in the performed order. Because inserting seams also changes the index of pixels in the original image, a matrix of original pixel index is stored, and the seam inserted is always on the index of the original image pixels. Thus, in the animation of the seam insertion, there will be times that the red line indicating seams became disconnected. This is because previous seams inserted have changed pixel positions in the image, and in order to insert corresponding seams, their pixel positions need to be adjusted too. The duplicated seam is the average of the three pixels surrounding it in the horizontal direction. Vertical Insertion (adding horizontal seams) is achieved similarly. </p>\n<h4 id=\"Some-Challenges\"><a href=\"#Some-Challenges\" class=\"headerlink\" title=\"Some Challenges\"></a>Some Challenges</h4><p>While implementing seam deletions, the most challenging aspect is to correctly set up dynamic program to find the seam. Because seam pixels need to be connected to previous pixels (can only be off by 1 pixel), when dynamically going through all pixels in an image, the range that the next step pixel can take is restricted to between +1 and -1 of the current pixel position. It also took some tries to figure out that using entropy as the energy function is relatively stable and easy to implement. Other functions that detect edges that I tried did not work as well in seam carving.<br>In my opinion, seam insertion is more challenging than seam carving. The intuitive solution is to find a seam, duplicate it, then find a new seam in the stretched image, and again duplicated it. This repeated seam insertion process, however, would not work because it would just be the same seam being duplicated again and again. Therefore, there is the need find at once set of all seams that would be removed if performing a seam deletion, then reverse-engineering these seams and duplicate each of them. It is also challenging to figure out how to correctly duplicate seams at correct pixels. The seams, if finding them the seam way as in seam deletion, would not work, because the image sizes have changed. Therefore, it is necessary to keep track of seams in the pixel index or positions in the original image’s perspective. And when inserting the duplicated seams, it is necessary again to use the original image’s indexes. </p>\n<p>The other challenging part of the project is to find out a way to minimize artifacts, especially on images where there is a specific object to keep intact. The solution I came up with is to use the getMask function that we have used in multiple other assignments. Using a user-specified black and white mask of the image, it is possible to increase energy on the masked regions when finding seams. Thus, the seam-finding algorithm will seek to avoid these regions.</p>\n<h4 id=\"Seam-Carving-and-Insertion-Processes\"><a href=\"#Seam-Carving-and-Insertion-Processes\" class=\"headerlink\" title=\"Seam Carving and Insertion Processes\"></a>Seam Carving and Insertion Processes</h4><p><a href=\"https://github.com/iasnobmatsu/SeamCarvingAndInsertion\">Code and more</a></p>\n<h4 id=\"Examples\"><a href=\"#Examples\" class=\"headerlink\" title=\"Examples\"></a>Examples</h4><p>original image</p>\n<img src=\"/2021/05/08/002-seam/set4_original.jpg\" class=\"\">\n<p>removing 90 pixels horizontally</p>\n<img src=\"/2021/05/08/002-seam/set4_carved3.jpg\" class=\"\">\n<p>inseting 30 pixels horizontally</p>\n<img src=\"/2021/05/08/002-seam/set4_carved7.jpg\" class=\"\">\n<p><a href=\"https://jojo.fandom.com/wiki/Hirohiko_Araki_JoJo_Exhibition_2012?file=Exhib8.jpg\">original image</a></p>\n<img src=\"/2021/05/08/002-seam/set1_original.jpg\" class=\"\">\n<p>removing 90 pixels vertically </p>\n<img src=\"/2021/05/08/002-seam/set1_carved6.jpg\" class=\"\">\n<p>inseting 30 pixels vertically</p>\n<img src=\"/2021/05/08/002-seam/set1_carved8.jpg\" class=\"\">"},{"title":"Web Scraping with Python Using MyAnimeList as An Example","date":"2020-12-30T05:00:00.000Z","_content":"\n\nDisclaimer: The following code for scraping MAL was written on Dec 30th, 2020. MAL data structure may have changed after that.\n\n<!-- To downlaod the ipynb (python jupyter notebook) script I wrote for this post, please click [here]({{site.baseurl}}/html_assets/MALscrape/MALscrapper.ipynb).\n\nTo download the MAL top 3000 anime list csv file (collected Dec 29, 2020), please click [here]({{site.baseurl}}/html_assets/MALscrape/MALtop3000.csv).\n\nTo download my own MAL anime list csv file (collected Dec 30, 2020), please click [here]({{site.baseurl}}/html_assets/MALscrape/iasnobmatsuMAL.csv). -->\n\n### Scraping Static HTML: Using MAL Top Animes as An Example\n\n#### Import libraries\n\n- BeautifulSoup: for scraping\n- requests: request html and parse\n- re: regular expression for string manipulation\n- pandas: convert data scraped into csv files\n\n<!-- more -->\n\n\n```python\nfrom bs4 import BeautifulSoup \nimport requests\nimport re\nimport pandas as pd\n```\n\n#### Helper Function to Parse One Anime Row\n\n![]({{site.baseurl}}/images/MALscrape/static.png)\n\nLooking at the html of [https://myanimelist.net/topanime.php](https://myanimelist.net/topanime.php) (using chrome, right click and select inspect, navigate to the element section, and you will see the HTML), each anime is a tr (table row) of the table. Within each row, name of anime is wrapped in class anime_ranking_h3, related information in class information, and score in class score. These can be scraped with beautifulsoup rather simply using the select() function. Then the text can be cleaned.\n\nWe can further get a show's start year and end year from the related information section. Here I used regular expression to get 4 digits of year to match start and end years.\n\n\n```python\ndef getOneRow(targetrow):\n    animeTitle=targetrow.select(\"h3.anime_ranking_h3\")[0].text\n    animeInformation=targetrow.select(\"div.information\")[0].text.replace(\"\\n\",\"|\").replace(\"  \",\"\")\n\n    animeScore=targetrow.select(\"td.score\")[0].text.replace(\"\\n\", \"\")\n    # split by |\n    year=animeInformation.split(\"|\") \n    # get all years in the second section from above\n    years=re.findall('[0-9]+', year[2])\n    start=\"NA\"\n    end=\"NA\"\n    \n    if len(year)>0:\n        start=years[0]\n        if len(years)>1:\n            end=years[1]\n    return animeTitle, animeInformation,animeScore, start, end\n\n# tablerow[0]\n```\n\n#### Function to Get a Specified Number of Anime on The Top Anime List\n\nPass in the url into requests.get() function to get the entire page, then make a soup out of it with BeautifulSoup. With the soup ready, we could find the table corresponding to the top anime list and find all its rows. For each row, get desired data with the getOneRow() helper function. Because each page of the top anime list only has 50 animes, if requesting more than 50 anime, make sure to get a loop to scrape pages after the first one.\n\n\n```python\ndef getTopAnime(limit):\n    # I find using a dict to store data is the easiest, and it's easy to convert to JSON or csv\n    topanimedict=[] \n\n    #url\n    url = \"https://myanimelist.net/topanime.php\" \n    #make soup of html\n    soup = BeautifulSoup(requests.get(url).text, 'lxml') \n    #get table corresponding to the top anime table.\n    toptable = soup.select(\"table\")[0] \n    #get all rows in the table\n    tablerow=toptable.select(\"tr.ranking-list\") \n     #get data for each row\n    for row in tablerow:\n        anime, info, score, st, ed=getOneRow(row)\n        tempdict={\"anime\": anime,\"start\": st, \"end\":ed,  \"score\": score, \"information\": info}\n        topanimedict.append(tempdict)\n        \n    # get page 2, 3, 4 etc after the first one\n    if limit>50: \n        ind=limit//50\n        for i in range (1,ind):\n            url = \"https://myanimelist.net/topanime.php?limit=\"+str(50*i)\n            print(url)\n            soup = BeautifulSoup(requests.get(url).text, 'lxml')\n            toptable = soup.select(\"table\")[0]\n            tablerow=toptable.select(\"tr.ranking-list\")\n            for row in tablerow:\n                anime, info, score, st, ed=getOneRow(row)\n                tempdict={\"anime\": anime,\"start\": st, \"end\":ed,  \"score\": score, \"information\": info}\n                topanimedict.append(tempdict)\n    \n    topanimedf=pd.DataFrame.from_dict(topanimedict)\n    return topanimedf\n\n```\n\n#### Convert Data\n\nWith the help of a dictionary and the pandas library, it is really easy to convert what we scraped into a csv. This script will save the data to the same directory where the script is stored.\n\n\n```python\ndf=getTopAnime(3000)\ndf.to_csv('MALtop3000.csv', index=False)\n```\n\n    https://myanimelist.net/topanime.php?limit=50\n    https://myanimelist.net/topanime.php?limit=100\n    https://myanimelist.net/topanime.php?limit=150\n    https://myanimelist.net/topanime.php?limit=200\n    https://myanimelist.net/topanime.php?limit=250\n    https://myanimelist.net/topanime.php?limit=300\n    https://myanimelist.net/topanime.php?limit=350\n    https://myanimelist.net/topanime.php?limit=400\n    https://myanimelist.net/topanime.php?limit=450\n    https://myanimelist.net/topanime.php?limit=500\n    https://myanimelist.net/topanime.php?limit=550\n    https://myanimelist.net/topanime.php?limit=600\n    https://myanimelist.net/topanime.php?limit=650\n    https://myanimelist.net/topanime.php?limit=700\n    https://myanimelist.net/topanime.php?limit=750\n    https://myanimelist.net/topanime.php?limit=800\n    https://myanimelist.net/topanime.php?limit=850\n    https://myanimelist.net/topanime.php?limit=900\n    https://myanimelist.net/topanime.php?limit=950\n    https://myanimelist.net/topanime.php?limit=1000\n    https://myanimelist.net/topanime.php?limit=1050\n    https://myanimelist.net/topanime.php?limit=1100\n    https://myanimelist.net/topanime.php?limit=1150\n    https://myanimelist.net/topanime.php?limit=1200\n    https://myanimelist.net/topanime.php?limit=1250\n    https://myanimelist.net/topanime.php?limit=1300\n    https://myanimelist.net/topanime.php?limit=1350\n    https://myanimelist.net/topanime.php?limit=1400\n    https://myanimelist.net/topanime.php?limit=1450\n    https://myanimelist.net/topanime.php?limit=1500\n    https://myanimelist.net/topanime.php?limit=1550\n    https://myanimelist.net/topanime.php?limit=1600\n    https://myanimelist.net/topanime.php?limit=1650\n    https://myanimelist.net/topanime.php?limit=1700\n    https://myanimelist.net/topanime.php?limit=1750\n    https://myanimelist.net/topanime.php?limit=1800\n    https://myanimelist.net/topanime.php?limit=1850\n    https://myanimelist.net/topanime.php?limit=1900\n    https://myanimelist.net/topanime.php?limit=1950\n    https://myanimelist.net/topanime.php?limit=2000\n    https://myanimelist.net/topanime.php?limit=2050\n    https://myanimelist.net/topanime.php?limit=2100\n    https://myanimelist.net/topanime.php?limit=2150\n    https://myanimelist.net/topanime.php?limit=2200\n    https://myanimelist.net/topanime.php?limit=2250\n    https://myanimelist.net/topanime.php?limit=2300\n    https://myanimelist.net/topanime.php?limit=2350\n    https://myanimelist.net/topanime.php?limit=2400\n    https://myanimelist.net/topanime.php?limit=2450\n    https://myanimelist.net/topanime.php?limit=2500\n    https://myanimelist.net/topanime.php?limit=2550\n    https://myanimelist.net/topanime.php?limit=2600\n    https://myanimelist.net/topanime.php?limit=2650\n    https://myanimelist.net/topanime.php?limit=2700\n    https://myanimelist.net/topanime.php?limit=2750\n    https://myanimelist.net/topanime.php?limit=2800\n    https://myanimelist.net/topanime.php?limit=2850\n    https://myanimelist.net/topanime.php?limit=2900\n    https://myanimelist.net/topanime.php?limit=2950\n\n\nTake a look at the scrape data file. Looked pretty neat to me. Index is the ranking-1.\n\n\n```python\ndf.tail()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>anime</th>\n      <th>end</th>\n      <th>information</th>\n      <th>score</th>\n      <th>start</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2995</th>\n      <td>Sekirei</td>\n      <td>2008</td>\n      <td>|TV (12 eps)|Jul 2008 - Sep 2008|320,922 members|</td>\n      <td>7.14</td>\n      <td>2008</td>\n    </tr>\n    <tr>\n      <th>2996</th>\n      <td>Shin Atashin'chi</td>\n      <td>2016</td>\n      <td>|TV (26 eps)|Oct 2015 - Apr 2016|2,427 members|</td>\n      <td>7.14</td>\n      <td>2015</td>\n    </tr>\n    <tr>\n      <th>2997</th>\n      <td>Tantei Opera Milky Holmes Movie: Gyakushuu no ...</td>\n      <td>2016</td>\n      <td>|Movie (1 eps)|Feb 2016 - Feb 2016|3,417 members|</td>\n      <td>7.14</td>\n      <td>2016</td>\n    </tr>\n    <tr>\n      <th>2998</th>\n      <td>Tenchi Muyou! Manatsu no Eve</td>\n      <td>1997</td>\n      <td>|Movie (1 eps)|Aug 1997 - Aug 1997|13,514 memb...</td>\n      <td>7.14</td>\n      <td>1997</td>\n    </tr>\n    <tr>\n      <th>2999</th>\n      <td>Tengen Toppa Gurren Lagann: Parallel Works</td>\n      <td>2008</td>\n      <td>|Music (8 eps)|Jun 2008 - Sep 2008|29,743 memb...</td>\n      <td>7.14</td>\n      <td>2008</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n### Scraping Dynamic HTML: Using MAL user list as An Example\n\nwith the code here, you will be able to scrape any user's MAL. Here I used my own anime list as an example ([https://myanimelist.net/animelist/iasnobmatsu](https://myanimelist.net/animelist/iasnobmatsu), FYI I highly highly recommend Attack on Titan, Haikyu, and Hoseki no Kuni).\n\nDynamic HTML is different from static HTML as the static HTML is rendered from HTML source file (imaging writing an html file and that is what we scrape). Dynamic HTML, on the other side, is not rendered from HTML source files but from JavaScript (Or JQuery or React, whatever framework). Dynamic HTML, unlike static, is not generate the moment a url is opened, but will need some time to render after the document is ready.\n\n#### Helper Function to Get One Row of MAL User List\n\n![]({{site.baseurl}}/images/MALscrape/dynamic.png)\n\nSimilar to the getOneRow function(), this function parses specific data for one anime. This step is the same regardless of static or dynamic HTML.\n\n\n\n```python\ndef getOneRowMAL(targetrow):\n    animeTitle=targetrow.select(\"td.title\")[0].select(\"a.link.sort\")[0].text\n    animeType=targetrow.select(\"td.type\")[0].text.strip()\n    animeScore=targetrow.select(\"td.score\")[0].text.strip()\n    animeProgress=targetrow.select(\"td.progress\")[0].text.replace(\"\\n\", \"\").replace(\"  \",\"\")\n    return animeTitle, animeType,animeScore, animeProgress\n\ngetOneRowMAL(rows[27])\n```\n\n\n\n\n    ('Haikyuu!!', 'TV', '7', ' 25 ')\n\n\n\n#### Additional Libraries for Dynamic HTML\n\nFor scraping dynamic HTML, we need selenium and time. \n\n\n```python\nfrom selenium import webdriver\nimport time\n```\n\n#### Get Dynamic MAL User List Data\n\nto scrape dynamic data, we need the url of the webpage. We also need to have a web browser driver. Here I use the Chrome driver (download here [https://chromedriver.chromium.org/](https://chromedriver.chromium.org/) or through homebrew etc). I stored it in my download folder, and I will need the path to the driver. I used Mac and Chrome driver in this case. \n\nWith the url of webpage and path to browser driver ready, we will use selenium to declare a driver variable, and use it instead of requests to get the url.\n\nThen it is important to delay the rest of the function by some time, here I used .2 but it may differ depend on how fast a page loads on a specific device under specific internet conditions. This time allows dynamic HTML to render so we scrape the desired content instead of the intial script used to generate the HTML (which we cannot parse).\nThen similar steps to scrape each row of data from the user anime list using BeautifulSoup.\n\nWhen using selenium with webdriver to scrap data, the browser may pop open with the url. You should not close the window until the data is scraped. If the window is closedbefore beautifulsoup get the change to read code on the driver, it will not work.\n\n\n```python\ndef getMAL(url, driverPath):\n    MALdict=[]\n    # use selenium to simulate driver\n    driver = webdriver.Chrome(driverPath)\n    driver.get(url) # get page\n\n    time.sleep(0.2) # may need to change\n    # similar to static, get soup and parse\n    soup=BeautifulSoup(driver.page_source, 'lxml')\n    toptable = soup.select(\"table\")[0]\n    rows=toptable.select(\"tbody.list-item\")\n    for row in rows:\n        ti,ty,sc,pr=getOneRowMAL(row)\n        MALdict.append({\"anime\":ti,\"type\":ty, \"score\":sc,\"progress\":pr})\n    return pd.DataFrame.from_dict(MALdict)\n\n\n```\n\n#### Convert Data\n\nHere we use the function above to get dynamic HTML data from my MAL list (you can replace with any user's MAL list. The data is saved again to a CSV file.\n\n\n```python\nurl='https://myanimelist.net/animelist/iasnobmatsu'\ndriverp=\"/Users/ziqianxu/Downloads/chromedriver\"\ndf2=getMAL(url,driverp)\ndf2.to_csv('iasnobmatsuMAL.csv', index=False)\ndf2.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>anime</th>\n      <th>progress</th>\n      <th>score</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>JoJo no Kimyou na Bouken Part 3: Stardust Crus...</td>\n      <td>- / 24</td>\n      <td>8</td>\n      <td>TV</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>One Piece</td>\n      <td>- / -</td>\n      <td>8</td>\n      <td>TV</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Shingeki no Kyojin: The Final Season</td>\n      <td>- / 16</td>\n      <td>10</td>\n      <td>TV</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Akagami no Shirayuki-hime</td>\n      <td>12</td>\n      <td>5</td>\n      <td>TV</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Bleach</td>\n      <td>366</td>\n      <td>7</td>\n      <td>TV</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n","source":"_posts/003-web-scraping.md","raw":"---\ntitle: Web Scraping with Python Using MyAnimeList as An Example\ndate: 2020-12-30\ncategories: \n- [data science]\ntags: [python, web scraping]\n---\n\n\nDisclaimer: The following code for scraping MAL was written on Dec 30th, 2020. MAL data structure may have changed after that.\n\n<!-- To downlaod the ipynb (python jupyter notebook) script I wrote for this post, please click [here]({{site.baseurl}}/html_assets/MALscrape/MALscrapper.ipynb).\n\nTo download the MAL top 3000 anime list csv file (collected Dec 29, 2020), please click [here]({{site.baseurl}}/html_assets/MALscrape/MALtop3000.csv).\n\nTo download my own MAL anime list csv file (collected Dec 30, 2020), please click [here]({{site.baseurl}}/html_assets/MALscrape/iasnobmatsuMAL.csv). -->\n\n### Scraping Static HTML: Using MAL Top Animes as An Example\n\n#### Import libraries\n\n- BeautifulSoup: for scraping\n- requests: request html and parse\n- re: regular expression for string manipulation\n- pandas: convert data scraped into csv files\n\n<!-- more -->\n\n\n```python\nfrom bs4 import BeautifulSoup \nimport requests\nimport re\nimport pandas as pd\n```\n\n#### Helper Function to Parse One Anime Row\n\n![]({{site.baseurl}}/images/MALscrape/static.png)\n\nLooking at the html of [https://myanimelist.net/topanime.php](https://myanimelist.net/topanime.php) (using chrome, right click and select inspect, navigate to the element section, and you will see the HTML), each anime is a tr (table row) of the table. Within each row, name of anime is wrapped in class anime_ranking_h3, related information in class information, and score in class score. These can be scraped with beautifulsoup rather simply using the select() function. Then the text can be cleaned.\n\nWe can further get a show's start year and end year from the related information section. Here I used regular expression to get 4 digits of year to match start and end years.\n\n\n```python\ndef getOneRow(targetrow):\n    animeTitle=targetrow.select(\"h3.anime_ranking_h3\")[0].text\n    animeInformation=targetrow.select(\"div.information\")[0].text.replace(\"\\n\",\"|\").replace(\"  \",\"\")\n\n    animeScore=targetrow.select(\"td.score\")[0].text.replace(\"\\n\", \"\")\n    # split by |\n    year=animeInformation.split(\"|\") \n    # get all years in the second section from above\n    years=re.findall('[0-9]+', year[2])\n    start=\"NA\"\n    end=\"NA\"\n    \n    if len(year)>0:\n        start=years[0]\n        if len(years)>1:\n            end=years[1]\n    return animeTitle, animeInformation,animeScore, start, end\n\n# tablerow[0]\n```\n\n#### Function to Get a Specified Number of Anime on The Top Anime List\n\nPass in the url into requests.get() function to get the entire page, then make a soup out of it with BeautifulSoup. With the soup ready, we could find the table corresponding to the top anime list and find all its rows. For each row, get desired data with the getOneRow() helper function. Because each page of the top anime list only has 50 animes, if requesting more than 50 anime, make sure to get a loop to scrape pages after the first one.\n\n\n```python\ndef getTopAnime(limit):\n    # I find using a dict to store data is the easiest, and it's easy to convert to JSON or csv\n    topanimedict=[] \n\n    #url\n    url = \"https://myanimelist.net/topanime.php\" \n    #make soup of html\n    soup = BeautifulSoup(requests.get(url).text, 'lxml') \n    #get table corresponding to the top anime table.\n    toptable = soup.select(\"table\")[0] \n    #get all rows in the table\n    tablerow=toptable.select(\"tr.ranking-list\") \n     #get data for each row\n    for row in tablerow:\n        anime, info, score, st, ed=getOneRow(row)\n        tempdict={\"anime\": anime,\"start\": st, \"end\":ed,  \"score\": score, \"information\": info}\n        topanimedict.append(tempdict)\n        \n    # get page 2, 3, 4 etc after the first one\n    if limit>50: \n        ind=limit//50\n        for i in range (1,ind):\n            url = \"https://myanimelist.net/topanime.php?limit=\"+str(50*i)\n            print(url)\n            soup = BeautifulSoup(requests.get(url).text, 'lxml')\n            toptable = soup.select(\"table\")[0]\n            tablerow=toptable.select(\"tr.ranking-list\")\n            for row in tablerow:\n                anime, info, score, st, ed=getOneRow(row)\n                tempdict={\"anime\": anime,\"start\": st, \"end\":ed,  \"score\": score, \"information\": info}\n                topanimedict.append(tempdict)\n    \n    topanimedf=pd.DataFrame.from_dict(topanimedict)\n    return topanimedf\n\n```\n\n#### Convert Data\n\nWith the help of a dictionary and the pandas library, it is really easy to convert what we scraped into a csv. This script will save the data to the same directory where the script is stored.\n\n\n```python\ndf=getTopAnime(3000)\ndf.to_csv('MALtop3000.csv', index=False)\n```\n\n    https://myanimelist.net/topanime.php?limit=50\n    https://myanimelist.net/topanime.php?limit=100\n    https://myanimelist.net/topanime.php?limit=150\n    https://myanimelist.net/topanime.php?limit=200\n    https://myanimelist.net/topanime.php?limit=250\n    https://myanimelist.net/topanime.php?limit=300\n    https://myanimelist.net/topanime.php?limit=350\n    https://myanimelist.net/topanime.php?limit=400\n    https://myanimelist.net/topanime.php?limit=450\n    https://myanimelist.net/topanime.php?limit=500\n    https://myanimelist.net/topanime.php?limit=550\n    https://myanimelist.net/topanime.php?limit=600\n    https://myanimelist.net/topanime.php?limit=650\n    https://myanimelist.net/topanime.php?limit=700\n    https://myanimelist.net/topanime.php?limit=750\n    https://myanimelist.net/topanime.php?limit=800\n    https://myanimelist.net/topanime.php?limit=850\n    https://myanimelist.net/topanime.php?limit=900\n    https://myanimelist.net/topanime.php?limit=950\n    https://myanimelist.net/topanime.php?limit=1000\n    https://myanimelist.net/topanime.php?limit=1050\n    https://myanimelist.net/topanime.php?limit=1100\n    https://myanimelist.net/topanime.php?limit=1150\n    https://myanimelist.net/topanime.php?limit=1200\n    https://myanimelist.net/topanime.php?limit=1250\n    https://myanimelist.net/topanime.php?limit=1300\n    https://myanimelist.net/topanime.php?limit=1350\n    https://myanimelist.net/topanime.php?limit=1400\n    https://myanimelist.net/topanime.php?limit=1450\n    https://myanimelist.net/topanime.php?limit=1500\n    https://myanimelist.net/topanime.php?limit=1550\n    https://myanimelist.net/topanime.php?limit=1600\n    https://myanimelist.net/topanime.php?limit=1650\n    https://myanimelist.net/topanime.php?limit=1700\n    https://myanimelist.net/topanime.php?limit=1750\n    https://myanimelist.net/topanime.php?limit=1800\n    https://myanimelist.net/topanime.php?limit=1850\n    https://myanimelist.net/topanime.php?limit=1900\n    https://myanimelist.net/topanime.php?limit=1950\n    https://myanimelist.net/topanime.php?limit=2000\n    https://myanimelist.net/topanime.php?limit=2050\n    https://myanimelist.net/topanime.php?limit=2100\n    https://myanimelist.net/topanime.php?limit=2150\n    https://myanimelist.net/topanime.php?limit=2200\n    https://myanimelist.net/topanime.php?limit=2250\n    https://myanimelist.net/topanime.php?limit=2300\n    https://myanimelist.net/topanime.php?limit=2350\n    https://myanimelist.net/topanime.php?limit=2400\n    https://myanimelist.net/topanime.php?limit=2450\n    https://myanimelist.net/topanime.php?limit=2500\n    https://myanimelist.net/topanime.php?limit=2550\n    https://myanimelist.net/topanime.php?limit=2600\n    https://myanimelist.net/topanime.php?limit=2650\n    https://myanimelist.net/topanime.php?limit=2700\n    https://myanimelist.net/topanime.php?limit=2750\n    https://myanimelist.net/topanime.php?limit=2800\n    https://myanimelist.net/topanime.php?limit=2850\n    https://myanimelist.net/topanime.php?limit=2900\n    https://myanimelist.net/topanime.php?limit=2950\n\n\nTake a look at the scrape data file. Looked pretty neat to me. Index is the ranking-1.\n\n\n```python\ndf.tail()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>anime</th>\n      <th>end</th>\n      <th>information</th>\n      <th>score</th>\n      <th>start</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2995</th>\n      <td>Sekirei</td>\n      <td>2008</td>\n      <td>|TV (12 eps)|Jul 2008 - Sep 2008|320,922 members|</td>\n      <td>7.14</td>\n      <td>2008</td>\n    </tr>\n    <tr>\n      <th>2996</th>\n      <td>Shin Atashin'chi</td>\n      <td>2016</td>\n      <td>|TV (26 eps)|Oct 2015 - Apr 2016|2,427 members|</td>\n      <td>7.14</td>\n      <td>2015</td>\n    </tr>\n    <tr>\n      <th>2997</th>\n      <td>Tantei Opera Milky Holmes Movie: Gyakushuu no ...</td>\n      <td>2016</td>\n      <td>|Movie (1 eps)|Feb 2016 - Feb 2016|3,417 members|</td>\n      <td>7.14</td>\n      <td>2016</td>\n    </tr>\n    <tr>\n      <th>2998</th>\n      <td>Tenchi Muyou! Manatsu no Eve</td>\n      <td>1997</td>\n      <td>|Movie (1 eps)|Aug 1997 - Aug 1997|13,514 memb...</td>\n      <td>7.14</td>\n      <td>1997</td>\n    </tr>\n    <tr>\n      <th>2999</th>\n      <td>Tengen Toppa Gurren Lagann: Parallel Works</td>\n      <td>2008</td>\n      <td>|Music (8 eps)|Jun 2008 - Sep 2008|29,743 memb...</td>\n      <td>7.14</td>\n      <td>2008</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n### Scraping Dynamic HTML: Using MAL user list as An Example\n\nwith the code here, you will be able to scrape any user's MAL. Here I used my own anime list as an example ([https://myanimelist.net/animelist/iasnobmatsu](https://myanimelist.net/animelist/iasnobmatsu), FYI I highly highly recommend Attack on Titan, Haikyu, and Hoseki no Kuni).\n\nDynamic HTML is different from static HTML as the static HTML is rendered from HTML source file (imaging writing an html file and that is what we scrape). Dynamic HTML, on the other side, is not rendered from HTML source files but from JavaScript (Or JQuery or React, whatever framework). Dynamic HTML, unlike static, is not generate the moment a url is opened, but will need some time to render after the document is ready.\n\n#### Helper Function to Get One Row of MAL User List\n\n![]({{site.baseurl}}/images/MALscrape/dynamic.png)\n\nSimilar to the getOneRow function(), this function parses specific data for one anime. This step is the same regardless of static or dynamic HTML.\n\n\n\n```python\ndef getOneRowMAL(targetrow):\n    animeTitle=targetrow.select(\"td.title\")[0].select(\"a.link.sort\")[0].text\n    animeType=targetrow.select(\"td.type\")[0].text.strip()\n    animeScore=targetrow.select(\"td.score\")[0].text.strip()\n    animeProgress=targetrow.select(\"td.progress\")[0].text.replace(\"\\n\", \"\").replace(\"  \",\"\")\n    return animeTitle, animeType,animeScore, animeProgress\n\ngetOneRowMAL(rows[27])\n```\n\n\n\n\n    ('Haikyuu!!', 'TV', '7', ' 25 ')\n\n\n\n#### Additional Libraries for Dynamic HTML\n\nFor scraping dynamic HTML, we need selenium and time. \n\n\n```python\nfrom selenium import webdriver\nimport time\n```\n\n#### Get Dynamic MAL User List Data\n\nto scrape dynamic data, we need the url of the webpage. We also need to have a web browser driver. Here I use the Chrome driver (download here [https://chromedriver.chromium.org/](https://chromedriver.chromium.org/) or through homebrew etc). I stored it in my download folder, and I will need the path to the driver. I used Mac and Chrome driver in this case. \n\nWith the url of webpage and path to browser driver ready, we will use selenium to declare a driver variable, and use it instead of requests to get the url.\n\nThen it is important to delay the rest of the function by some time, here I used .2 but it may differ depend on how fast a page loads on a specific device under specific internet conditions. This time allows dynamic HTML to render so we scrape the desired content instead of the intial script used to generate the HTML (which we cannot parse).\nThen similar steps to scrape each row of data from the user anime list using BeautifulSoup.\n\nWhen using selenium with webdriver to scrap data, the browser may pop open with the url. You should not close the window until the data is scraped. If the window is closedbefore beautifulsoup get the change to read code on the driver, it will not work.\n\n\n```python\ndef getMAL(url, driverPath):\n    MALdict=[]\n    # use selenium to simulate driver\n    driver = webdriver.Chrome(driverPath)\n    driver.get(url) # get page\n\n    time.sleep(0.2) # may need to change\n    # similar to static, get soup and parse\n    soup=BeautifulSoup(driver.page_source, 'lxml')\n    toptable = soup.select(\"table\")[0]\n    rows=toptable.select(\"tbody.list-item\")\n    for row in rows:\n        ti,ty,sc,pr=getOneRowMAL(row)\n        MALdict.append({\"anime\":ti,\"type\":ty, \"score\":sc,\"progress\":pr})\n    return pd.DataFrame.from_dict(MALdict)\n\n\n```\n\n#### Convert Data\n\nHere we use the function above to get dynamic HTML data from my MAL list (you can replace with any user's MAL list. The data is saved again to a CSV file.\n\n\n```python\nurl='https://myanimelist.net/animelist/iasnobmatsu'\ndriverp=\"/Users/ziqianxu/Downloads/chromedriver\"\ndf2=getMAL(url,driverp)\ndf2.to_csv('iasnobmatsuMAL.csv', index=False)\ndf2.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>anime</th>\n      <th>progress</th>\n      <th>score</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>JoJo no Kimyou na Bouken Part 3: Stardust Crus...</td>\n      <td>- / 24</td>\n      <td>8</td>\n      <td>TV</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>One Piece</td>\n      <td>- / -</td>\n      <td>8</td>\n      <td>TV</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Shingeki no Kyojin: The Final Season</td>\n      <td>- / 16</td>\n      <td>10</td>\n      <td>TV</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Akagami no Shirayuki-hime</td>\n      <td>12</td>\n      <td>5</td>\n      <td>TV</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Bleach</td>\n      <td>366</td>\n      <td>7</td>\n      <td>TV</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n","slug":"003-web-scraping","published":1,"updated":"2022-11-05T16:35:45.837Z","_id":"cla38j5c00004mxvg4i1798p6","comments":1,"layout":"post","photos":[],"link":"","content":"<p>Disclaimer: The following code for scraping MAL was written on Dec 30th, 2020. MAL data structure may have changed after that.</p>\n<!-- To downlaod the ipynb (python jupyter notebook) script I wrote for this post, please click [here](/html_assets/MALscrape/MALscrapper.ipynb).\n\nTo download the MAL top 3000 anime list csv file (collected Dec 29, 2020), please click [here](/html_assets/MALscrape/MALtop3000.csv).\n\nTo download my own MAL anime list csv file (collected Dec 30, 2020), please click [here](/html_assets/MALscrape/iasnobmatsuMAL.csv). -->\n<h3 id=\"Scraping-Static-HTML-Using-MAL-Top-Animes-as-An-Example\"><a href=\"#Scraping-Static-HTML-Using-MAL-Top-Animes-as-An-Example\" class=\"headerlink\" title=\"Scraping Static HTML: Using MAL Top Animes as An Example\"></a>Scraping Static HTML: Using MAL Top Animes as An Example</h3><h4 id=\"Import-libraries\"><a href=\"#Import-libraries\" class=\"headerlink\" title=\"Import libraries\"></a>Import libraries</h4><ul>\n<li>BeautifulSoup: for scraping</li>\n<li>requests: request html and parse</li>\n<li>re: regular expression for string manipulation</li>\n<li>pandas: convert data scraped into csv files</li>\n</ul>\n<span id=\"more\"></span>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> bs4 <span class=\"keyword\">import</span> BeautifulSoup </span><br><span class=\"line\"><span class=\"keyword\">import</span> requests</span><br><span class=\"line\"><span class=\"keyword\">import</span> re</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br></pre></td></tr></table></figure>\n<h4 id=\"Helper-Function-to-Parse-One-Anime-Row\"><a href=\"#Helper-Function-to-Parse-One-Anime-Row\" class=\"headerlink\" title=\"Helper Function to Parse One Anime Row\"></a>Helper Function to Parse One Anime Row</h4><p><img src=\"!--swig￼12--&gt;/images/MALscrape/static.png\" alt=\"\"></p>\n<p>Looking at the html of <a href=\"https://myanimelist.net/topanime.php\">https://myanimelist.net/topanime.php</a> (using chrome, right click and select inspect, navigate to the element section, and you will see the HTML), each anime is a tr (table row) of the table. Within each row, name of anime is wrapped in class anime_ranking_h3, related information in class information, and score in class score. These can be scraped with beautifulsoup rather simply using the select() function. Then the text can be cleaned.</p>\n<p>We can further get a show’s start year and end year from the related information section. Here I used regular expression to get 4 digits of year to match start and end years.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">getOneRow</span>(<span class=\"params\">targetrow</span>):</span><br><span class=\"line\">    animeTitle=targetrow.select(<span class=\"string\">&quot;h3.anime_ranking_h3&quot;</span>)[<span class=\"number\">0</span>].text</span><br><span class=\"line\">    animeInformation=targetrow.select(<span class=\"string\">&quot;div.information&quot;</span>)[<span class=\"number\">0</span>].text.replace(<span class=\"string\">&quot;\\n&quot;</span>,<span class=\"string\">&quot;|&quot;</span>).replace(<span class=\"string\">&quot;  &quot;</span>,<span class=\"string\">&quot;&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    animeScore=targetrow.select(<span class=\"string\">&quot;td.score&quot;</span>)[<span class=\"number\">0</span>].text.replace(<span class=\"string\">&quot;\\n&quot;</span>, <span class=\"string\">&quot;&quot;</span>)</span><br><span class=\"line\">    <span class=\"comment\"># split by |</span></span><br><span class=\"line\">    year=animeInformation.split(<span class=\"string\">&quot;|&quot;</span>) </span><br><span class=\"line\">    <span class=\"comment\"># get all years in the second section from above</span></span><br><span class=\"line\">    years=re.findall(<span class=\"string\">&#x27;[0-9]+&#x27;</span>, year[<span class=\"number\">2</span>])</span><br><span class=\"line\">    start=<span class=\"string\">&quot;NA&quot;</span></span><br><span class=\"line\">    end=<span class=\"string\">&quot;NA&quot;</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(year)&gt;<span class=\"number\">0</span>:</span><br><span class=\"line\">        start=years[<span class=\"number\">0</span>]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(years)&gt;<span class=\"number\">1</span>:</span><br><span class=\"line\">            end=years[<span class=\"number\">1</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> animeTitle, animeInformation,animeScore, start, end</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># tablerow[0]</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"Function-to-Get-a-Specified-Number-of-Anime-on-The-Top-Anime-List\"><a href=\"#Function-to-Get-a-Specified-Number-of-Anime-on-The-Top-Anime-List\" class=\"headerlink\" title=\"Function to Get a Specified Number of Anime on The Top Anime List\"></a>Function to Get a Specified Number of Anime on The Top Anime List</h4><p>Pass in the url into requests.get() function to get the entire page, then make a soup out of it with BeautifulSoup. With the soup ready, we could find the table corresponding to the top anime list and find all its rows. For each row, get desired data with the getOneRow() helper function. Because each page of the top anime list only has 50 animes, if requesting more than 50 anime, make sure to get a loop to scrape pages after the first one.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">getTopAnime</span>(<span class=\"params\">limit</span>):</span><br><span class=\"line\">    <span class=\"comment\"># I find using a dict to store data is the easiest, and it&#x27;s easy to convert to JSON or csv</span></span><br><span class=\"line\">    topanimedict=[] </span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">#url</span></span><br><span class=\"line\">    url = <span class=\"string\">&quot;https://myanimelist.net/topanime.php&quot;</span> </span><br><span class=\"line\">    <span class=\"comment\">#make soup of html</span></span><br><span class=\"line\">    soup = BeautifulSoup(requests.get(url).text, <span class=\"string\">&#x27;lxml&#x27;</span>) </span><br><span class=\"line\">    <span class=\"comment\">#get table corresponding to the top anime table.</span></span><br><span class=\"line\">    toptable = soup.select(<span class=\"string\">&quot;table&quot;</span>)[<span class=\"number\">0</span>] </span><br><span class=\"line\">    <span class=\"comment\">#get all rows in the table</span></span><br><span class=\"line\">    tablerow=toptable.select(<span class=\"string\">&quot;tr.ranking-list&quot;</span>) </span><br><span class=\"line\">     <span class=\"comment\">#get data for each row</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> row <span class=\"keyword\">in</span> tablerow:</span><br><span class=\"line\">        anime, info, score, st, ed=getOneRow(row)</span><br><span class=\"line\">        tempdict=&#123;<span class=\"string\">&quot;anime&quot;</span>: anime,<span class=\"string\">&quot;start&quot;</span>: st, <span class=\"string\">&quot;end&quot;</span>:ed,  <span class=\"string\">&quot;score&quot;</span>: score, <span class=\"string\">&quot;information&quot;</span>: info&#125;</span><br><span class=\"line\">        topanimedict.append(tempdict)</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"comment\"># get page 2, 3, 4 etc after the first one</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> limit&gt;<span class=\"number\">50</span>: </span><br><span class=\"line\">        ind=limit//<span class=\"number\">50</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span> (<span class=\"number\">1</span>,ind):</span><br><span class=\"line\">            url = <span class=\"string\">&quot;https://myanimelist.net/topanime.php?limit=&quot;</span>+<span class=\"built_in\">str</span>(<span class=\"number\">50</span>*i)</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(url)</span><br><span class=\"line\">            soup = BeautifulSoup(requests.get(url).text, <span class=\"string\">&#x27;lxml&#x27;</span>)</span><br><span class=\"line\">            toptable = soup.select(<span class=\"string\">&quot;table&quot;</span>)[<span class=\"number\">0</span>]</span><br><span class=\"line\">            tablerow=toptable.select(<span class=\"string\">&quot;tr.ranking-list&quot;</span>)</span><br><span class=\"line\">            <span class=\"keyword\">for</span> row <span class=\"keyword\">in</span> tablerow:</span><br><span class=\"line\">                anime, info, score, st, ed=getOneRow(row)</span><br><span class=\"line\">                tempdict=&#123;<span class=\"string\">&quot;anime&quot;</span>: anime,<span class=\"string\">&quot;start&quot;</span>: st, <span class=\"string\">&quot;end&quot;</span>:ed,  <span class=\"string\">&quot;score&quot;</span>: score, <span class=\"string\">&quot;information&quot;</span>: info&#125;</span><br><span class=\"line\">                topanimedict.append(tempdict)</span><br><span class=\"line\">    </span><br><span class=\"line\">    topanimedf=pd.DataFrame.from_dict(topanimedict)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> topanimedf</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h4 id=\"Convert-Data\"><a href=\"#Convert-Data\" class=\"headerlink\" title=\"Convert Data\"></a>Convert Data</h4><p>With the help of a dictionary and the pandas library, it is really easy to convert what we scraped into a csv. This script will save the data to the same directory where the script is stored.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df=getTopAnime(<span class=\"number\">3000</span>)</span><br><span class=\"line\">df.to_csv(<span class=\"string\">&#x27;MALtop3000.csv&#x27;</span>, index=<span class=\"literal\">False</span>)</span><br></pre></td></tr></table></figure>\n<pre><code>https://myanimelist.net/topanime.php?limit=50\nhttps://myanimelist.net/topanime.php?limit=100\nhttps://myanimelist.net/topanime.php?limit=150\nhttps://myanimelist.net/topanime.php?limit=200\nhttps://myanimelist.net/topanime.php?limit=250\nhttps://myanimelist.net/topanime.php?limit=300\nhttps://myanimelist.net/topanime.php?limit=350\nhttps://myanimelist.net/topanime.php?limit=400\nhttps://myanimelist.net/topanime.php?limit=450\nhttps://myanimelist.net/topanime.php?limit=500\nhttps://myanimelist.net/topanime.php?limit=550\nhttps://myanimelist.net/topanime.php?limit=600\nhttps://myanimelist.net/topanime.php?limit=650\nhttps://myanimelist.net/topanime.php?limit=700\nhttps://myanimelist.net/topanime.php?limit=750\nhttps://myanimelist.net/topanime.php?limit=800\nhttps://myanimelist.net/topanime.php?limit=850\nhttps://myanimelist.net/topanime.php?limit=900\nhttps://myanimelist.net/topanime.php?limit=950\nhttps://myanimelist.net/topanime.php?limit=1000\nhttps://myanimelist.net/topanime.php?limit=1050\nhttps://myanimelist.net/topanime.php?limit=1100\nhttps://myanimelist.net/topanime.php?limit=1150\nhttps://myanimelist.net/topanime.php?limit=1200\nhttps://myanimelist.net/topanime.php?limit=1250\nhttps://myanimelist.net/topanime.php?limit=1300\nhttps://myanimelist.net/topanime.php?limit=1350\nhttps://myanimelist.net/topanime.php?limit=1400\nhttps://myanimelist.net/topanime.php?limit=1450\nhttps://myanimelist.net/topanime.php?limit=1500\nhttps://myanimelist.net/topanime.php?limit=1550\nhttps://myanimelist.net/topanime.php?limit=1600\nhttps://myanimelist.net/topanime.php?limit=1650\nhttps://myanimelist.net/topanime.php?limit=1700\nhttps://myanimelist.net/topanime.php?limit=1750\nhttps://myanimelist.net/topanime.php?limit=1800\nhttps://myanimelist.net/topanime.php?limit=1850\nhttps://myanimelist.net/topanime.php?limit=1900\nhttps://myanimelist.net/topanime.php?limit=1950\nhttps://myanimelist.net/topanime.php?limit=2000\nhttps://myanimelist.net/topanime.php?limit=2050\nhttps://myanimelist.net/topanime.php?limit=2100\nhttps://myanimelist.net/topanime.php?limit=2150\nhttps://myanimelist.net/topanime.php?limit=2200\nhttps://myanimelist.net/topanime.php?limit=2250\nhttps://myanimelist.net/topanime.php?limit=2300\nhttps://myanimelist.net/topanime.php?limit=2350\nhttps://myanimelist.net/topanime.php?limit=2400\nhttps://myanimelist.net/topanime.php?limit=2450\nhttps://myanimelist.net/topanime.php?limit=2500\nhttps://myanimelist.net/topanime.php?limit=2550\nhttps://myanimelist.net/topanime.php?limit=2600\nhttps://myanimelist.net/topanime.php?limit=2650\nhttps://myanimelist.net/topanime.php?limit=2700\nhttps://myanimelist.net/topanime.php?limit=2750\nhttps://myanimelist.net/topanime.php?limit=2800\nhttps://myanimelist.net/topanime.php?limit=2850\nhttps://myanimelist.net/topanime.php?limit=2900\nhttps://myanimelist.net/topanime.php?limit=2950\n</code></pre><p>Take a look at the scrape data file. Looked pretty neat to me. Index is the ranking-1.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df.tail()</span><br></pre></td></tr></table></figure>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>anime</th>\n      <th>end</th>\n      <th>information</th>\n      <th>score</th>\n      <th>start</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2995</th>\n      <td>Sekirei</td>\n      <td>2008</td>\n      <td>|TV (12 eps)|Jul 2008 - Sep 2008|320,922 members|</td>\n      <td>7.14</td>\n      <td>2008</td>\n    </tr>\n    <tr>\n      <th>2996</th>\n      <td>Shin Atashin'chi</td>\n      <td>2016</td>\n      <td>|TV (26 eps)|Oct 2015 - Apr 2016|2,427 members|</td>\n      <td>7.14</td>\n      <td>2015</td>\n    </tr>\n    <tr>\n      <th>2997</th>\n      <td>Tantei Opera Milky Holmes Movie: Gyakushuu no ...</td>\n      <td>2016</td>\n      <td>|Movie (1 eps)|Feb 2016 - Feb 2016|3,417 members|</td>\n      <td>7.14</td>\n      <td>2016</td>\n    </tr>\n    <tr>\n      <th>2998</th>\n      <td>Tenchi Muyou! Manatsu no Eve</td>\n      <td>1997</td>\n      <td>|Movie (1 eps)|Aug 1997 - Aug 1997|13,514 memb...</td>\n      <td>7.14</td>\n      <td>1997</td>\n    </tr>\n    <tr>\n      <th>2999</th>\n      <td>Tengen Toppa Gurren Lagann: Parallel Works</td>\n      <td>2008</td>\n      <td>|Music (8 eps)|Jun 2008 - Sep 2008|29,743 memb...</td>\n      <td>7.14</td>\n      <td>2008</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n<h3 id=\"Scraping-Dynamic-HTML-Using-MAL-user-list-as-An-Example\"><a href=\"#Scraping-Dynamic-HTML-Using-MAL-user-list-as-An-Example\" class=\"headerlink\" title=\"Scraping Dynamic HTML: Using MAL user list as An Example\"></a>Scraping Dynamic HTML: Using MAL user list as An Example</h3><p>with the code here, you will be able to scrape any user’s MAL. Here I used my own anime list as an example (<a href=\"https://myanimelist.net/animelist/iasnobmatsu\">https://myanimelist.net/animelist/iasnobmatsu</a>, FYI I highly highly recommend Attack on Titan, Haikyu, and Hoseki no Kuni).</p>\n<p>Dynamic HTML is different from static HTML as the static HTML is rendered from HTML source file (imaging writing an html file and that is what we scrape). Dynamic HTML, on the other side, is not rendered from HTML source files but from JavaScript (Or JQuery or React, whatever framework). Dynamic HTML, unlike static, is not generate the moment a url is opened, but will need some time to render after the document is ready.</p>\n<h4 id=\"Helper-Function-to-Get-One-Row-of-MAL-User-List\"><a href=\"#Helper-Function-to-Get-One-Row-of-MAL-User-List\" class=\"headerlink\" title=\"Helper Function to Get One Row of MAL User List\"></a>Helper Function to Get One Row of MAL User List</h4><p><img src=\"!--swig￼13--&gt;/images/MALscrape/dynamic.png\" alt=\"\"></p>\n<p>Similar to the getOneRow function(), this function parses specific data for one anime. This step is the same regardless of static or dynamic HTML.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">getOneRowMAL</span>(<span class=\"params\">targetrow</span>):</span><br><span class=\"line\">    animeTitle=targetrow.select(<span class=\"string\">&quot;td.title&quot;</span>)[<span class=\"number\">0</span>].select(<span class=\"string\">&quot;a.link.sort&quot;</span>)[<span class=\"number\">0</span>].text</span><br><span class=\"line\">    animeType=targetrow.select(<span class=\"string\">&quot;td.type&quot;</span>)[<span class=\"number\">0</span>].text.strip()</span><br><span class=\"line\">    animeScore=targetrow.select(<span class=\"string\">&quot;td.score&quot;</span>)[<span class=\"number\">0</span>].text.strip()</span><br><span class=\"line\">    animeProgress=targetrow.select(<span class=\"string\">&quot;td.progress&quot;</span>)[<span class=\"number\">0</span>].text.replace(<span class=\"string\">&quot;\\n&quot;</span>, <span class=\"string\">&quot;&quot;</span>).replace(<span class=\"string\">&quot;  &quot;</span>,<span class=\"string\">&quot;&quot;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> animeTitle, animeType,animeScore, animeProgress</span><br><span class=\"line\"></span><br><span class=\"line\">getOneRowMAL(rows[<span class=\"number\">27</span>])</span><br></pre></td></tr></table></figure>\n<pre><code>(&#39;Haikyuu!!&#39;, &#39;TV&#39;, &#39;7&#39;, &#39; 25 &#39;)\n</code></pre><h4 id=\"Additional-Libraries-for-Dynamic-HTML\"><a href=\"#Additional-Libraries-for-Dynamic-HTML\" class=\"headerlink\" title=\"Additional Libraries for Dynamic HTML\"></a>Additional Libraries for Dynamic HTML</h4><p>For scraping dynamic HTML, we need selenium and time. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> selenium <span class=\"keyword\">import</span> webdriver</span><br><span class=\"line\"><span class=\"keyword\">import</span> time</span><br></pre></td></tr></table></figure>\n<h4 id=\"Get-Dynamic-MAL-User-List-Data\"><a href=\"#Get-Dynamic-MAL-User-List-Data\" class=\"headerlink\" title=\"Get Dynamic MAL User List Data\"></a>Get Dynamic MAL User List Data</h4><p>to scrape dynamic data, we need the url of the webpage. We also need to have a web browser driver. Here I use the Chrome driver (download here <a href=\"https://chromedriver.chromium.org/\">https://chromedriver.chromium.org/</a> or through homebrew etc). I stored it in my download folder, and I will need the path to the driver. I used Mac and Chrome driver in this case. </p>\n<p>With the url of webpage and path to browser driver ready, we will use selenium to declare a driver variable, and use it instead of requests to get the url.</p>\n<p>Then it is important to delay the rest of the function by some time, here I used .2 but it may differ depend on how fast a page loads on a specific device under specific internet conditions. This time allows dynamic HTML to render so we scrape the desired content instead of the intial script used to generate the HTML (which we cannot parse).<br>Then similar steps to scrape each row of data from the user anime list using BeautifulSoup.</p>\n<p>When using selenium with webdriver to scrap data, the browser may pop open with the url. You should not close the window until the data is scraped. If the window is closedbefore beautifulsoup get the change to read code on the driver, it will not work.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">getMAL</span>(<span class=\"params\">url, driverPath</span>):</span><br><span class=\"line\">    MALdict=[]</span><br><span class=\"line\">    <span class=\"comment\"># use selenium to simulate driver</span></span><br><span class=\"line\">    driver = webdriver.Chrome(driverPath)</span><br><span class=\"line\">    driver.get(url) <span class=\"comment\"># get page</span></span><br><span class=\"line\"></span><br><span class=\"line\">    time.sleep(<span class=\"number\">0.2</span>) <span class=\"comment\"># may need to change</span></span><br><span class=\"line\">    <span class=\"comment\"># similar to static, get soup and parse</span></span><br><span class=\"line\">    soup=BeautifulSoup(driver.page_source, <span class=\"string\">&#x27;lxml&#x27;</span>)</span><br><span class=\"line\">    toptable = soup.select(<span class=\"string\">&quot;table&quot;</span>)[<span class=\"number\">0</span>]</span><br><span class=\"line\">    rows=toptable.select(<span class=\"string\">&quot;tbody.list-item&quot;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> row <span class=\"keyword\">in</span> rows:</span><br><span class=\"line\">        ti,ty,sc,pr=getOneRowMAL(row)</span><br><span class=\"line\">        MALdict.append(&#123;<span class=\"string\">&quot;anime&quot;</span>:ti,<span class=\"string\">&quot;type&quot;</span>:ty, <span class=\"string\">&quot;score&quot;</span>:sc,<span class=\"string\">&quot;progress&quot;</span>:pr&#125;)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> pd.DataFrame.from_dict(MALdict)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h4 id=\"Convert-Data-1\"><a href=\"#Convert-Data-1\" class=\"headerlink\" title=\"Convert Data\"></a>Convert Data</h4><p>Here we use the function above to get dynamic HTML data from my MAL list (you can replace with any user’s MAL list. The data is saved again to a CSV file.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">url=<span class=\"string\">&#x27;https://myanimelist.net/animelist/iasnobmatsu&#x27;</span></span><br><span class=\"line\">driverp=<span class=\"string\">&quot;/Users/ziqianxu/Downloads/chromedriver&quot;</span></span><br><span class=\"line\">df2=getMAL(url,driverp)</span><br><span class=\"line\">df2.to_csv(<span class=\"string\">&#x27;iasnobmatsuMAL.csv&#x27;</span>, index=<span class=\"literal\">False</span>)</span><br><span class=\"line\">df2.head()</span><br></pre></td></tr></table></figure>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>anime</th>\n      <th>progress</th>\n      <th>score</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>JoJo no Kimyou na Bouken Part 3: Stardust Crus...</td>\n      <td>- / 24</td>\n      <td>8</td>\n      <td>TV</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>One Piece</td>\n      <td>- / -</td>\n      <td>8</td>\n      <td>TV</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Shingeki no Kyojin: The Final Season</td>\n      <td>- / 16</td>\n      <td>10</td>\n      <td>TV</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Akagami no Shirayuki-hime</td>\n      <td>12</td>\n      <td>5</td>\n      <td>TV</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Bleach</td>\n      <td>366</td>\n      <td>7</td>\n      <td>TV</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n","site":{"data":{}},"excerpt":"<p>Disclaimer: The following code for scraping MAL was written on Dec 30th, 2020. MAL data structure may have changed after that.</p>\n<!-- To downlaod the ipynb (python jupyter notebook) script I wrote for this post, please click [here](/html_assets/MALscrape/MALscrapper.ipynb).\n\nTo download the MAL top 3000 anime list csv file (collected Dec 29, 2020), please click [here](/html_assets/MALscrape/MALtop3000.csv).\n\nTo download my own MAL anime list csv file (collected Dec 30, 2020), please click [here](/html_assets/MALscrape/iasnobmatsuMAL.csv). -->\n<h3 id=\"Scraping-Static-HTML-Using-MAL-Top-Animes-as-An-Example\"><a href=\"#Scraping-Static-HTML-Using-MAL-Top-Animes-as-An-Example\" class=\"headerlink\" title=\"Scraping Static HTML: Using MAL Top Animes as An Example\"></a>Scraping Static HTML: Using MAL Top Animes as An Example</h3><h4 id=\"Import-libraries\"><a href=\"#Import-libraries\" class=\"headerlink\" title=\"Import libraries\"></a>Import libraries</h4><ul>\n<li>BeautifulSoup: for scraping</li>\n<li>requests: request html and parse</li>\n<li>re: regular expression for string manipulation</li>\n<li>pandas: convert data scraped into csv files</li>\n</ul>","more":"<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> bs4 <span class=\"keyword\">import</span> BeautifulSoup </span><br><span class=\"line\"><span class=\"keyword\">import</span> requests</span><br><span class=\"line\"><span class=\"keyword\">import</span> re</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br></pre></td></tr></table></figure>\n<h4 id=\"Helper-Function-to-Parse-One-Anime-Row\"><a href=\"#Helper-Function-to-Parse-One-Anime-Row\" class=\"headerlink\" title=\"Helper Function to Parse One Anime Row\"></a>Helper Function to Parse One Anime Row</h4><p><img src=\"!--swig￼12--&gt;/images/MALscrape/static.png\" alt=\"\"></p>\n<p>Looking at the html of <a href=\"https://myanimelist.net/topanime.php\">https://myanimelist.net/topanime.php</a> (using chrome, right click and select inspect, navigate to the element section, and you will see the HTML), each anime is a tr (table row) of the table. Within each row, name of anime is wrapped in class anime_ranking_h3, related information in class information, and score in class score. These can be scraped with beautifulsoup rather simply using the select() function. Then the text can be cleaned.</p>\n<p>We can further get a show’s start year and end year from the related information section. Here I used regular expression to get 4 digits of year to match start and end years.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">getOneRow</span>(<span class=\"params\">targetrow</span>):</span><br><span class=\"line\">    animeTitle=targetrow.select(<span class=\"string\">&quot;h3.anime_ranking_h3&quot;</span>)[<span class=\"number\">0</span>].text</span><br><span class=\"line\">    animeInformation=targetrow.select(<span class=\"string\">&quot;div.information&quot;</span>)[<span class=\"number\">0</span>].text.replace(<span class=\"string\">&quot;\\n&quot;</span>,<span class=\"string\">&quot;|&quot;</span>).replace(<span class=\"string\">&quot;  &quot;</span>,<span class=\"string\">&quot;&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    animeScore=targetrow.select(<span class=\"string\">&quot;td.score&quot;</span>)[<span class=\"number\">0</span>].text.replace(<span class=\"string\">&quot;\\n&quot;</span>, <span class=\"string\">&quot;&quot;</span>)</span><br><span class=\"line\">    <span class=\"comment\"># split by |</span></span><br><span class=\"line\">    year=animeInformation.split(<span class=\"string\">&quot;|&quot;</span>) </span><br><span class=\"line\">    <span class=\"comment\"># get all years in the second section from above</span></span><br><span class=\"line\">    years=re.findall(<span class=\"string\">&#x27;[0-9]+&#x27;</span>, year[<span class=\"number\">2</span>])</span><br><span class=\"line\">    start=<span class=\"string\">&quot;NA&quot;</span></span><br><span class=\"line\">    end=<span class=\"string\">&quot;NA&quot;</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(year)&gt;<span class=\"number\">0</span>:</span><br><span class=\"line\">        start=years[<span class=\"number\">0</span>]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(years)&gt;<span class=\"number\">1</span>:</span><br><span class=\"line\">            end=years[<span class=\"number\">1</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> animeTitle, animeInformation,animeScore, start, end</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># tablerow[0]</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"Function-to-Get-a-Specified-Number-of-Anime-on-The-Top-Anime-List\"><a href=\"#Function-to-Get-a-Specified-Number-of-Anime-on-The-Top-Anime-List\" class=\"headerlink\" title=\"Function to Get a Specified Number of Anime on The Top Anime List\"></a>Function to Get a Specified Number of Anime on The Top Anime List</h4><p>Pass in the url into requests.get() function to get the entire page, then make a soup out of it with BeautifulSoup. With the soup ready, we could find the table corresponding to the top anime list and find all its rows. For each row, get desired data with the getOneRow() helper function. Because each page of the top anime list only has 50 animes, if requesting more than 50 anime, make sure to get a loop to scrape pages after the first one.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">getTopAnime</span>(<span class=\"params\">limit</span>):</span><br><span class=\"line\">    <span class=\"comment\"># I find using a dict to store data is the easiest, and it&#x27;s easy to convert to JSON or csv</span></span><br><span class=\"line\">    topanimedict=[] </span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">#url</span></span><br><span class=\"line\">    url = <span class=\"string\">&quot;https://myanimelist.net/topanime.php&quot;</span> </span><br><span class=\"line\">    <span class=\"comment\">#make soup of html</span></span><br><span class=\"line\">    soup = BeautifulSoup(requests.get(url).text, <span class=\"string\">&#x27;lxml&#x27;</span>) </span><br><span class=\"line\">    <span class=\"comment\">#get table corresponding to the top anime table.</span></span><br><span class=\"line\">    toptable = soup.select(<span class=\"string\">&quot;table&quot;</span>)[<span class=\"number\">0</span>] </span><br><span class=\"line\">    <span class=\"comment\">#get all rows in the table</span></span><br><span class=\"line\">    tablerow=toptable.select(<span class=\"string\">&quot;tr.ranking-list&quot;</span>) </span><br><span class=\"line\">     <span class=\"comment\">#get data for each row</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> row <span class=\"keyword\">in</span> tablerow:</span><br><span class=\"line\">        anime, info, score, st, ed=getOneRow(row)</span><br><span class=\"line\">        tempdict=&#123;<span class=\"string\">&quot;anime&quot;</span>: anime,<span class=\"string\">&quot;start&quot;</span>: st, <span class=\"string\">&quot;end&quot;</span>:ed,  <span class=\"string\">&quot;score&quot;</span>: score, <span class=\"string\">&quot;information&quot;</span>: info&#125;</span><br><span class=\"line\">        topanimedict.append(tempdict)</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"comment\"># get page 2, 3, 4 etc after the first one</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> limit&gt;<span class=\"number\">50</span>: </span><br><span class=\"line\">        ind=limit//<span class=\"number\">50</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span> (<span class=\"number\">1</span>,ind):</span><br><span class=\"line\">            url = <span class=\"string\">&quot;https://myanimelist.net/topanime.php?limit=&quot;</span>+<span class=\"built_in\">str</span>(<span class=\"number\">50</span>*i)</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(url)</span><br><span class=\"line\">            soup = BeautifulSoup(requests.get(url).text, <span class=\"string\">&#x27;lxml&#x27;</span>)</span><br><span class=\"line\">            toptable = soup.select(<span class=\"string\">&quot;table&quot;</span>)[<span class=\"number\">0</span>]</span><br><span class=\"line\">            tablerow=toptable.select(<span class=\"string\">&quot;tr.ranking-list&quot;</span>)</span><br><span class=\"line\">            <span class=\"keyword\">for</span> row <span class=\"keyword\">in</span> tablerow:</span><br><span class=\"line\">                anime, info, score, st, ed=getOneRow(row)</span><br><span class=\"line\">                tempdict=&#123;<span class=\"string\">&quot;anime&quot;</span>: anime,<span class=\"string\">&quot;start&quot;</span>: st, <span class=\"string\">&quot;end&quot;</span>:ed,  <span class=\"string\">&quot;score&quot;</span>: score, <span class=\"string\">&quot;information&quot;</span>: info&#125;</span><br><span class=\"line\">                topanimedict.append(tempdict)</span><br><span class=\"line\">    </span><br><span class=\"line\">    topanimedf=pd.DataFrame.from_dict(topanimedict)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> topanimedf</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h4 id=\"Convert-Data\"><a href=\"#Convert-Data\" class=\"headerlink\" title=\"Convert Data\"></a>Convert Data</h4><p>With the help of a dictionary and the pandas library, it is really easy to convert what we scraped into a csv. This script will save the data to the same directory where the script is stored.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df=getTopAnime(<span class=\"number\">3000</span>)</span><br><span class=\"line\">df.to_csv(<span class=\"string\">&#x27;MALtop3000.csv&#x27;</span>, index=<span class=\"literal\">False</span>)</span><br></pre></td></tr></table></figure>\n<pre><code>https://myanimelist.net/topanime.php?limit=50\nhttps://myanimelist.net/topanime.php?limit=100\nhttps://myanimelist.net/topanime.php?limit=150\nhttps://myanimelist.net/topanime.php?limit=200\nhttps://myanimelist.net/topanime.php?limit=250\nhttps://myanimelist.net/topanime.php?limit=300\nhttps://myanimelist.net/topanime.php?limit=350\nhttps://myanimelist.net/topanime.php?limit=400\nhttps://myanimelist.net/topanime.php?limit=450\nhttps://myanimelist.net/topanime.php?limit=500\nhttps://myanimelist.net/topanime.php?limit=550\nhttps://myanimelist.net/topanime.php?limit=600\nhttps://myanimelist.net/topanime.php?limit=650\nhttps://myanimelist.net/topanime.php?limit=700\nhttps://myanimelist.net/topanime.php?limit=750\nhttps://myanimelist.net/topanime.php?limit=800\nhttps://myanimelist.net/topanime.php?limit=850\nhttps://myanimelist.net/topanime.php?limit=900\nhttps://myanimelist.net/topanime.php?limit=950\nhttps://myanimelist.net/topanime.php?limit=1000\nhttps://myanimelist.net/topanime.php?limit=1050\nhttps://myanimelist.net/topanime.php?limit=1100\nhttps://myanimelist.net/topanime.php?limit=1150\nhttps://myanimelist.net/topanime.php?limit=1200\nhttps://myanimelist.net/topanime.php?limit=1250\nhttps://myanimelist.net/topanime.php?limit=1300\nhttps://myanimelist.net/topanime.php?limit=1350\nhttps://myanimelist.net/topanime.php?limit=1400\nhttps://myanimelist.net/topanime.php?limit=1450\nhttps://myanimelist.net/topanime.php?limit=1500\nhttps://myanimelist.net/topanime.php?limit=1550\nhttps://myanimelist.net/topanime.php?limit=1600\nhttps://myanimelist.net/topanime.php?limit=1650\nhttps://myanimelist.net/topanime.php?limit=1700\nhttps://myanimelist.net/topanime.php?limit=1750\nhttps://myanimelist.net/topanime.php?limit=1800\nhttps://myanimelist.net/topanime.php?limit=1850\nhttps://myanimelist.net/topanime.php?limit=1900\nhttps://myanimelist.net/topanime.php?limit=1950\nhttps://myanimelist.net/topanime.php?limit=2000\nhttps://myanimelist.net/topanime.php?limit=2050\nhttps://myanimelist.net/topanime.php?limit=2100\nhttps://myanimelist.net/topanime.php?limit=2150\nhttps://myanimelist.net/topanime.php?limit=2200\nhttps://myanimelist.net/topanime.php?limit=2250\nhttps://myanimelist.net/topanime.php?limit=2300\nhttps://myanimelist.net/topanime.php?limit=2350\nhttps://myanimelist.net/topanime.php?limit=2400\nhttps://myanimelist.net/topanime.php?limit=2450\nhttps://myanimelist.net/topanime.php?limit=2500\nhttps://myanimelist.net/topanime.php?limit=2550\nhttps://myanimelist.net/topanime.php?limit=2600\nhttps://myanimelist.net/topanime.php?limit=2650\nhttps://myanimelist.net/topanime.php?limit=2700\nhttps://myanimelist.net/topanime.php?limit=2750\nhttps://myanimelist.net/topanime.php?limit=2800\nhttps://myanimelist.net/topanime.php?limit=2850\nhttps://myanimelist.net/topanime.php?limit=2900\nhttps://myanimelist.net/topanime.php?limit=2950\n</code></pre><p>Take a look at the scrape data file. Looked pretty neat to me. Index is the ranking-1.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df.tail()</span><br></pre></td></tr></table></figure>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>anime</th>\n      <th>end</th>\n      <th>information</th>\n      <th>score</th>\n      <th>start</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2995</th>\n      <td>Sekirei</td>\n      <td>2008</td>\n      <td>|TV (12 eps)|Jul 2008 - Sep 2008|320,922 members|</td>\n      <td>7.14</td>\n      <td>2008</td>\n    </tr>\n    <tr>\n      <th>2996</th>\n      <td>Shin Atashin'chi</td>\n      <td>2016</td>\n      <td>|TV (26 eps)|Oct 2015 - Apr 2016|2,427 members|</td>\n      <td>7.14</td>\n      <td>2015</td>\n    </tr>\n    <tr>\n      <th>2997</th>\n      <td>Tantei Opera Milky Holmes Movie: Gyakushuu no ...</td>\n      <td>2016</td>\n      <td>|Movie (1 eps)|Feb 2016 - Feb 2016|3,417 members|</td>\n      <td>7.14</td>\n      <td>2016</td>\n    </tr>\n    <tr>\n      <th>2998</th>\n      <td>Tenchi Muyou! Manatsu no Eve</td>\n      <td>1997</td>\n      <td>|Movie (1 eps)|Aug 1997 - Aug 1997|13,514 memb...</td>\n      <td>7.14</td>\n      <td>1997</td>\n    </tr>\n    <tr>\n      <th>2999</th>\n      <td>Tengen Toppa Gurren Lagann: Parallel Works</td>\n      <td>2008</td>\n      <td>|Music (8 eps)|Jun 2008 - Sep 2008|29,743 memb...</td>\n      <td>7.14</td>\n      <td>2008</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n<h3 id=\"Scraping-Dynamic-HTML-Using-MAL-user-list-as-An-Example\"><a href=\"#Scraping-Dynamic-HTML-Using-MAL-user-list-as-An-Example\" class=\"headerlink\" title=\"Scraping Dynamic HTML: Using MAL user list as An Example\"></a>Scraping Dynamic HTML: Using MAL user list as An Example</h3><p>with the code here, you will be able to scrape any user’s MAL. Here I used my own anime list as an example (<a href=\"https://myanimelist.net/animelist/iasnobmatsu\">https://myanimelist.net/animelist/iasnobmatsu</a>, FYI I highly highly recommend Attack on Titan, Haikyu, and Hoseki no Kuni).</p>\n<p>Dynamic HTML is different from static HTML as the static HTML is rendered from HTML source file (imaging writing an html file and that is what we scrape). Dynamic HTML, on the other side, is not rendered from HTML source files but from JavaScript (Or JQuery or React, whatever framework). Dynamic HTML, unlike static, is not generate the moment a url is opened, but will need some time to render after the document is ready.</p>\n<h4 id=\"Helper-Function-to-Get-One-Row-of-MAL-User-List\"><a href=\"#Helper-Function-to-Get-One-Row-of-MAL-User-List\" class=\"headerlink\" title=\"Helper Function to Get One Row of MAL User List\"></a>Helper Function to Get One Row of MAL User List</h4><p><img src=\"!--swig￼13--&gt;/images/MALscrape/dynamic.png\" alt=\"\"></p>\n<p>Similar to the getOneRow function(), this function parses specific data for one anime. This step is the same regardless of static or dynamic HTML.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">getOneRowMAL</span>(<span class=\"params\">targetrow</span>):</span><br><span class=\"line\">    animeTitle=targetrow.select(<span class=\"string\">&quot;td.title&quot;</span>)[<span class=\"number\">0</span>].select(<span class=\"string\">&quot;a.link.sort&quot;</span>)[<span class=\"number\">0</span>].text</span><br><span class=\"line\">    animeType=targetrow.select(<span class=\"string\">&quot;td.type&quot;</span>)[<span class=\"number\">0</span>].text.strip()</span><br><span class=\"line\">    animeScore=targetrow.select(<span class=\"string\">&quot;td.score&quot;</span>)[<span class=\"number\">0</span>].text.strip()</span><br><span class=\"line\">    animeProgress=targetrow.select(<span class=\"string\">&quot;td.progress&quot;</span>)[<span class=\"number\">0</span>].text.replace(<span class=\"string\">&quot;\\n&quot;</span>, <span class=\"string\">&quot;&quot;</span>).replace(<span class=\"string\">&quot;  &quot;</span>,<span class=\"string\">&quot;&quot;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> animeTitle, animeType,animeScore, animeProgress</span><br><span class=\"line\"></span><br><span class=\"line\">getOneRowMAL(rows[<span class=\"number\">27</span>])</span><br></pre></td></tr></table></figure>\n<pre><code>(&#39;Haikyuu!!&#39;, &#39;TV&#39;, &#39;7&#39;, &#39; 25 &#39;)\n</code></pre><h4 id=\"Additional-Libraries-for-Dynamic-HTML\"><a href=\"#Additional-Libraries-for-Dynamic-HTML\" class=\"headerlink\" title=\"Additional Libraries for Dynamic HTML\"></a>Additional Libraries for Dynamic HTML</h4><p>For scraping dynamic HTML, we need selenium and time. </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> selenium <span class=\"keyword\">import</span> webdriver</span><br><span class=\"line\"><span class=\"keyword\">import</span> time</span><br></pre></td></tr></table></figure>\n<h4 id=\"Get-Dynamic-MAL-User-List-Data\"><a href=\"#Get-Dynamic-MAL-User-List-Data\" class=\"headerlink\" title=\"Get Dynamic MAL User List Data\"></a>Get Dynamic MAL User List Data</h4><p>to scrape dynamic data, we need the url of the webpage. We also need to have a web browser driver. Here I use the Chrome driver (download here <a href=\"https://chromedriver.chromium.org/\">https://chromedriver.chromium.org/</a> or through homebrew etc). I stored it in my download folder, and I will need the path to the driver. I used Mac and Chrome driver in this case. </p>\n<p>With the url of webpage and path to browser driver ready, we will use selenium to declare a driver variable, and use it instead of requests to get the url.</p>\n<p>Then it is important to delay the rest of the function by some time, here I used .2 but it may differ depend on how fast a page loads on a specific device under specific internet conditions. This time allows dynamic HTML to render so we scrape the desired content instead of the intial script used to generate the HTML (which we cannot parse).<br>Then similar steps to scrape each row of data from the user anime list using BeautifulSoup.</p>\n<p>When using selenium with webdriver to scrap data, the browser may pop open with the url. You should not close the window until the data is scraped. If the window is closedbefore beautifulsoup get the change to read code on the driver, it will not work.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">getMAL</span>(<span class=\"params\">url, driverPath</span>):</span><br><span class=\"line\">    MALdict=[]</span><br><span class=\"line\">    <span class=\"comment\"># use selenium to simulate driver</span></span><br><span class=\"line\">    driver = webdriver.Chrome(driverPath)</span><br><span class=\"line\">    driver.get(url) <span class=\"comment\"># get page</span></span><br><span class=\"line\"></span><br><span class=\"line\">    time.sleep(<span class=\"number\">0.2</span>) <span class=\"comment\"># may need to change</span></span><br><span class=\"line\">    <span class=\"comment\"># similar to static, get soup and parse</span></span><br><span class=\"line\">    soup=BeautifulSoup(driver.page_source, <span class=\"string\">&#x27;lxml&#x27;</span>)</span><br><span class=\"line\">    toptable = soup.select(<span class=\"string\">&quot;table&quot;</span>)[<span class=\"number\">0</span>]</span><br><span class=\"line\">    rows=toptable.select(<span class=\"string\">&quot;tbody.list-item&quot;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> row <span class=\"keyword\">in</span> rows:</span><br><span class=\"line\">        ti,ty,sc,pr=getOneRowMAL(row)</span><br><span class=\"line\">        MALdict.append(&#123;<span class=\"string\">&quot;anime&quot;</span>:ti,<span class=\"string\">&quot;type&quot;</span>:ty, <span class=\"string\">&quot;score&quot;</span>:sc,<span class=\"string\">&quot;progress&quot;</span>:pr&#125;)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> pd.DataFrame.from_dict(MALdict)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h4 id=\"Convert-Data-1\"><a href=\"#Convert-Data-1\" class=\"headerlink\" title=\"Convert Data\"></a>Convert Data</h4><p>Here we use the function above to get dynamic HTML data from my MAL list (you can replace with any user’s MAL list. The data is saved again to a CSV file.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">url=<span class=\"string\">&#x27;https://myanimelist.net/animelist/iasnobmatsu&#x27;</span></span><br><span class=\"line\">driverp=<span class=\"string\">&quot;/Users/ziqianxu/Downloads/chromedriver&quot;</span></span><br><span class=\"line\">df2=getMAL(url,driverp)</span><br><span class=\"line\">df2.to_csv(<span class=\"string\">&#x27;iasnobmatsuMAL.csv&#x27;</span>, index=<span class=\"literal\">False</span>)</span><br><span class=\"line\">df2.head()</span><br></pre></td></tr></table></figure>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>anime</th>\n      <th>progress</th>\n      <th>score</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>JoJo no Kimyou na Bouken Part 3: Stardust Crus...</td>\n      <td>- / 24</td>\n      <td>8</td>\n      <td>TV</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>One Piece</td>\n      <td>- / -</td>\n      <td>8</td>\n      <td>TV</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Shingeki no Kyojin: The Final Season</td>\n      <td>- / 16</td>\n      <td>10</td>\n      <td>TV</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Akagami no Shirayuki-hime</td>\n      <td>12</td>\n      <td>5</td>\n      <td>TV</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Bleach</td>\n      <td>366</td>\n      <td>7</td>\n      <td>TV</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},{"title":"Personal Favorites of Chinese Rock/Indie Music（一些我听的乐队）","date":"2022-11-05T16:36:13.000Z","hidden":false,"_content":"\n### 关于我听的歌\n\n我喜欢摇滚乐。最爱的有Led Zeppelin和Red Hot Chili Peppers。最近几年开始听中文的摇滚乐/indie歌曲。\n\n### 喜欢的乐队\n\nBeyond，康士坦的变化球，刺猬，傻子与白痴，草东没有派对，deca joins，以及很多别的。\n\n### 假如要选出今年最喜欢的五首歌\n\n那大概是\n\n- 冷雨夜(Beyond)\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/track/3ZovyLqQDxnTs3RbklLaa4?utm_source=generator\" width=\"100%\" height=\"82\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n- 美好的事可以可以发生在我身上(康士坦的变化球)\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/track/1viPIivqn5vmMsFUk5CJnw?utm_source=generator\" width=\"100%\" height=\"82\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n- 生之向往(刺猬)\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/track/3nBD3Qge1Sbk0G2bELhR82?utm_source=generator\" width=\"100%\" height=\"82\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n- 浴室(deca joins)\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/track/2Xu6fN0gE2S04L0zywmcif?utm_source=generator\" width=\"100%\" height=\"82\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n- 大雨(娃娃, cover by deca joins)\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/track/3k1IJYrxcQvm9m7UbITf3d?utm_source=generator\" width=\"100%\" height=\"82\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n\n### 歌单\n\n关于喜欢的歌，我有一个[Spotify 歌单](https://open.spotify.com/playlist/3P07vHgIgWP9HoH1g3Q5xT?si=e543d4ff403e4152)。\n\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/playlist/3P07vHgIgWP9HoH1g3Q5xT?utm_source=generator\" width=\"100%\" height=\"380\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n","source":"_posts/004-songs.md","raw":"---\ntitle: Personal Favorites of Chinese Rock/Indie Music（一些我听的乐队）\ndate: 2022-11-05 12:36:13\ntags: [摇滚乐, 歌单, rock, playlist, Chinese indie music]\ncategories: \n    - [music]\nhidden: false\n---\n\n### 关于我听的歌\n\n我喜欢摇滚乐。最爱的有Led Zeppelin和Red Hot Chili Peppers。最近几年开始听中文的摇滚乐/indie歌曲。\n\n### 喜欢的乐队\n\nBeyond，康士坦的变化球，刺猬，傻子与白痴，草东没有派对，deca joins，以及很多别的。\n\n### 假如要选出今年最喜欢的五首歌\n\n那大概是\n\n- 冷雨夜(Beyond)\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/track/3ZovyLqQDxnTs3RbklLaa4?utm_source=generator\" width=\"100%\" height=\"82\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n- 美好的事可以可以发生在我身上(康士坦的变化球)\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/track/1viPIivqn5vmMsFUk5CJnw?utm_source=generator\" width=\"100%\" height=\"82\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n- 生之向往(刺猬)\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/track/3nBD3Qge1Sbk0G2bELhR82?utm_source=generator\" width=\"100%\" height=\"82\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n- 浴室(deca joins)\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/track/2Xu6fN0gE2S04L0zywmcif?utm_source=generator\" width=\"100%\" height=\"82\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n- 大雨(娃娃, cover by deca joins)\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/track/3k1IJYrxcQvm9m7UbITf3d?utm_source=generator\" width=\"100%\" height=\"82\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n\n### 歌单\n\n关于喜欢的歌，我有一个[Spotify 歌单](https://open.spotify.com/playlist/3P07vHgIgWP9HoH1g3Q5xT?si=e543d4ff403e4152)。\n\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/playlist/3P07vHgIgWP9HoH1g3Q5xT?utm_source=generator\" width=\"100%\" height=\"380\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n","slug":"004-songs","published":1,"updated":"2022-11-05T19:35:16.973Z","_id":"cla45hdnj0000o0vg2l3x5mny","comments":1,"layout":"post","photos":[],"link":"","content":"<h3 id=\"关于我听的歌\"><a href=\"#关于我听的歌\" class=\"headerlink\" title=\"关于我听的歌\"></a>关于我听的歌</h3><p>我喜欢摇滚乐。最爱的有Led Zeppelin和Red Hot Chili Peppers。最近几年开始听中文的摇滚乐/indie歌曲。</p>\n<h3 id=\"喜欢的乐队\"><a href=\"#喜欢的乐队\" class=\"headerlink\" title=\"喜欢的乐队\"></a>喜欢的乐队</h3><p>Beyond，康士坦的变化球，刺猬，傻子与白痴，草东没有派对，deca joins，以及很多别的。</p>\n<h3 id=\"假如要选出今年最喜欢的五首歌\"><a href=\"#假如要选出今年最喜欢的五首歌\" class=\"headerlink\" title=\"假如要选出今年最喜欢的五首歌\"></a>假如要选出今年最喜欢的五首歌</h3><p>那大概是</p>\n<ul>\n<li>冷雨夜(Beyond)<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/track/3ZovyLqQDxnTs3RbklLaa4?utm_source=generator\" width=\"100%\" height=\"82\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe></li>\n<li>美好的事可以可以发生在我身上(康士坦的变化球)<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/track/1viPIivqn5vmMsFUk5CJnw?utm_source=generator\" width=\"100%\" height=\"82\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe></li>\n<li>生之向往(刺猬)<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/track/3nBD3Qge1Sbk0G2bELhR82?utm_source=generator\" width=\"100%\" height=\"82\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe></li>\n<li>浴室(deca joins)<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/track/2Xu6fN0gE2S04L0zywmcif?utm_source=generator\" width=\"100%\" height=\"82\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe></li>\n<li>大雨(娃娃, cover by deca joins)<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/track/3k1IJYrxcQvm9m7UbITf3d?utm_source=generator\" width=\"100%\" height=\"82\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n\n</li>\n</ul>\n<h3 id=\"歌单\"><a href=\"#歌单\" class=\"headerlink\" title=\"歌单\"></a>歌单</h3><p>关于喜欢的歌，我有一个<a href=\"https://open.spotify.com/playlist/3P07vHgIgWP9HoH1g3Q5xT?si=e543d4ff403e4152\">Spotify 歌单</a>。</p>\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/playlist/3P07vHgIgWP9HoH1g3Q5xT?utm_source=generator\" width=\"100%\" height=\"380\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"关于我听的歌\"><a href=\"#关于我听的歌\" class=\"headerlink\" title=\"关于我听的歌\"></a>关于我听的歌</h3><p>我喜欢摇滚乐。最爱的有Led Zeppelin和Red Hot Chili Peppers。最近几年开始听中文的摇滚乐/indie歌曲。</p>\n<h3 id=\"喜欢的乐队\"><a href=\"#喜欢的乐队\" class=\"headerlink\" title=\"喜欢的乐队\"></a>喜欢的乐队</h3><p>Beyond，康士坦的变化球，刺猬，傻子与白痴，草东没有派对，deca joins，以及很多别的。</p>\n<h3 id=\"假如要选出今年最喜欢的五首歌\"><a href=\"#假如要选出今年最喜欢的五首歌\" class=\"headerlink\" title=\"假如要选出今年最喜欢的五首歌\"></a>假如要选出今年最喜欢的五首歌</h3><p>那大概是</p>\n<ul>\n<li>冷雨夜(Beyond)<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/track/3ZovyLqQDxnTs3RbklLaa4?utm_source=generator\" width=\"100%\" height=\"82\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe></li>\n<li>美好的事可以可以发生在我身上(康士坦的变化球)<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/track/1viPIivqn5vmMsFUk5CJnw?utm_source=generator\" width=\"100%\" height=\"82\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe></li>\n<li>生之向往(刺猬)<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/track/3nBD3Qge1Sbk0G2bELhR82?utm_source=generator\" width=\"100%\" height=\"82\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe></li>\n<li>浴室(deca joins)<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/track/2Xu6fN0gE2S04L0zywmcif?utm_source=generator\" width=\"100%\" height=\"82\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe></li>\n<li>大雨(娃娃, cover by deca joins)<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/track/3k1IJYrxcQvm9m7UbITf3d?utm_source=generator\" width=\"100%\" height=\"82\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n\n</li>\n</ul>\n<h3 id=\"歌单\"><a href=\"#歌单\" class=\"headerlink\" title=\"歌单\"></a>歌单</h3><p>关于喜欢的歌，我有一个<a href=\"https://open.spotify.com/playlist/3P07vHgIgWP9HoH1g3Q5xT?si=e543d4ff403e4152\">Spotify 歌单</a>。</p>\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/playlist/3P07vHgIgWP9HoH1g3Q5xT?utm_source=generator\" width=\"100%\" height=\"380\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n"},{"title":"Steps in Conducting a Regresssion Analysis","date":"2022-11-05T19:16:20.000Z","hidden":true,"_content":"\nA summary of the workflow in conducying a linear regression.\n\n## Overview\n\n- Exploratory analysis of the data\n- Build the model\n- Residual analysis & assumption check\n- Make a conclusion\n- Model selection","source":"_posts/005-regression.md","raw":"---\ntitle: Steps in Conducting a Regresssion Analysis\ndate: 2022-11-05 15:16:20\ntags: [linear regression]\ncategories: [data science]\nhidden: true\n---\n\nA summary of the workflow in conducying a linear regression.\n\n## Overview\n\n- Exploratory analysis of the data\n- Build the model\n- Residual analysis & assumption check\n- Make a conclusion\n- Model selection","slug":"005-regression","published":1,"updated":"2022-11-05T19:35:10.717Z","_id":"cla4beqg1000036vgejo7gxg8","comments":1,"layout":"post","photos":[],"link":"","content":"<p>A summary of the workflow in conducying a linear regression.</p>\n<h2 id=\"Overview\"><a href=\"#Overview\" class=\"headerlink\" title=\"Overview\"></a>Overview</h2><ul>\n<li>Exploratory analysis of the data</li>\n<li>Build the model</li>\n<li>Residual analysis &amp; assumption check</li>\n<li>Make a conclusion</li>\n<li>Model selection</li>\n</ul>\n<!-- flag of hidden posts -->","site":{"data":{}},"excerpt":"","more":"<p>A summary of the workflow in conducying a linear regression.</p>\n<h2 id=\"Overview\"><a href=\"#Overview\" class=\"headerlink\" title=\"Overview\"></a>Overview</h2><ul>\n<li>Exploratory analysis of the data</li>\n<li>Build the model</li>\n<li>Residual analysis &amp; assumption check</li>\n<li>Make a conclusion</li>\n<li>Model selection</li>\n</ul>\n"},{"title":"Missing Data Imputation Under the Bayesian Framework Using JAGS","date":"2022-11-05T19:35:41.000Z","hidden":true,"_content":"","source":"_posts/006-missing-data-jags.md","raw":"---\ntitle: Missing Data Imputation Under the Bayesian Framework Using JAGS\ndate: 2022-11-05 15:35:41\ntags: [missing data, bayesian, JAGS]\ncategories: [data science]\nhidden: true\n---\n","slug":"006-missing-data-jags","published":1,"updated":"2022-11-05T19:45:16.594Z","_id":"cla4bteu00000chvgg0nq0odx","comments":1,"layout":"post","photos":[],"link":"","content":"<!-- flag of hidden posts -->","site":{"data":{}},"excerpt":"","more":""}],"PostAsset":[{"_id":"source/_posts/002-seam/seam-carving-demonstration-video.gif","slug":"seam-carving-demonstration-video.gif","post":"cla38j5bz0003mxvgh6ok3n65","modified":0,"renderable":0},{"_id":"source/_posts/002-seam/set1_carved6.jpg","slug":"set1_carved6.jpg","post":"cla38j5bz0003mxvgh6ok3n65","modified":0,"renderable":0},{"_id":"source/_posts/002-seam/set1_carved8.jpg","slug":"set1_carved8.jpg","post":"cla38j5bz0003mxvgh6ok3n65","modified":0,"renderable":0},{"_id":"source/_posts/002-seam/set1_original.jpg","slug":"set1_original.jpg","post":"cla38j5bz0003mxvgh6ok3n65","modified":0,"renderable":0},{"_id":"source/_posts/002-seam/set4_carved3.jpg","slug":"set4_carved3.jpg","post":"cla38j5bz0003mxvgh6ok3n65","modified":0,"renderable":0},{"_id":"source/_posts/002-seam/set4_carved7.jpg","slug":"set4_carved7.jpg","post":"cla38j5bz0003mxvgh6ok3n65","modified":0,"renderable":0},{"_id":"source/_posts/002-seam/set4_original.jpg","slug":"set4_original.jpg","post":"cla38j5bz0003mxvgh6ok3n65","modified":0,"renderable":0}],"PostCategory":[{"post_id":"cla38j5by0002mxvg346qcj1v","category_id":"cla38j5c00005mxvg4aa07k35","_id":"cla38j5c2000amxvgh3dm9oic"},{"post_id":"cla38j5bz0003mxvgh6ok3n65","category_id":"cla45czwu0000ccvghc13evpd","_id":"cla45czwx0001ccvg2irbc0q4"},{"post_id":"cla38j5c00004mxvg4i1798p6","category_id":"cla45czwu0000ccvghc13evpd","_id":"cla45d5ri0002ccvgd8sr39o2"},{"post_id":"cla45hdnj0000o0vg2l3x5mny","category_id":"cla45x13u0000y8vg5bjj9w23","_id":"cla45x13v0001y8vgg8d4fc3g"},{"post_id":"cla4beqg1000036vgejo7gxg8","category_id":"cla45czwu0000ccvghc13evpd","_id":"cla4bkjx7000136vgh11j3xy7"},{"post_id":"cla4bteu00000chvgg0nq0odx","category_id":"cla45czwu0000ccvghc13evpd","_id":"cla4bteu20002chvg0b8u5t9f"}],"PostTag":[{"post_id":"cla38j5bz0003mxvgh6ok3n65","tag_id":"cla38j5c10006mxvg1t534v9b","_id":"cla38j5c2000cmxvg8nz2fe7p"},{"post_id":"cla38j5bz0003mxvgh6ok3n65","tag_id":"cla38j5c10008mxvg5k4t0vai","_id":"cla38j5c2000dmxvg57ljc61p"},{"post_id":"cla38j5c00004mxvg4i1798p6","tag_id":"cla38j5c2000bmxvgggs335bq","_id":"cla38j5c2000hmxvgfyx460p8"},{"post_id":"cla38j5c00004mxvg4i1798p6","tag_id":"cla38j5c2000fmxvg75wwbx6k","_id":"cla38j5c2000imxvgeuvrc3k0"},{"post_id":"cla45hdnj0000o0vg2l3x5mny","tag_id":"cla45hdnj0001o0vg3goj6thk","_id":"cla45hdnk0003o0vgeo7b821c"},{"post_id":"cla45hdnj0000o0vg2l3x5mny","tag_id":"cla45hdnk0002o0vgcam5ccyw","_id":"cla45hdnl0004o0vg7kcge5dh"},{"post_id":"cla45hdnj0000o0vg2l3x5mny","tag_id":"cla45ieqo0006o0vgdh0a6wa6","_id":"cla45ieqp0009o0vg2afbhkwh"},{"post_id":"cla45hdnj0000o0vg2l3x5mny","tag_id":"cla45qgdf000bo0vg4fl1ajx4","_id":"cla45qgdg000co0vgdudkd322"},{"post_id":"cla45hdnj0000o0vg2l3x5mny","tag_id":"cla45qlbs000do0vge6di88ft","_id":"cla45qlbt000eo0vgeep5dgft"},{"post_id":"cla4beqg1000036vgejo7gxg8","tag_id":"cla4bkz4o000236vg7s3i1ssb","_id":"cla4bkz4q000336vg6dyce1il"},{"post_id":"cla4bteu00000chvgg0nq0odx","tag_id":"cla4bteu10001chvgak2ogdwb","_id":"cla4bteu30005chvge2iu2s73"},{"post_id":"cla4bteu00000chvgg0nq0odx","tag_id":"cla4bteu20003chvg0bw8bqo5","_id":"cla4bteu30006chvg8rru6197"},{"post_id":"cla4bteu00000chvgg0nq0odx","tag_id":"cla4bteu20004chvge28kbqkr","_id":"cla4bteu30007chvg3zoa5tgg"}],"Tag":[{"name":"computational photography","_id":"cla38j5c10006mxvg1t534v9b"},{"name":"matlab","_id":"cla38j5c10008mxvg5k4t0vai"},{"name":"python","_id":"cla38j5c2000bmxvgggs335bq"},{"name":"web scraping","_id":"cla38j5c2000fmxvg75wwbx6k"},{"name":"摇滚乐","_id":"cla45hdnj0001o0vg3goj6thk"},{"name":"歌单","_id":"cla45hdnk0002o0vgcam5ccyw"},{"name":"songs","_id":"cla45ieqm0005o0vg0pnh376j"},{"name":"playlist","_id":"cla45ieqo0006o0vgdh0a6wa6"},{"name":"Chinese indie","_id":"cla45ieqo0007o0vg309k18da"},{"name":"Chinese indie music","_id":"cla45qgdf000bo0vg4fl1ajx4"},{"name":"rock","_id":"cla45qlbs000do0vge6di88ft"},{"name":"linear regression","_id":"cla4bkz4o000236vg7s3i1ssb"},{"name":"missing data","_id":"cla4bteu10001chvgak2ogdwb"},{"name":"bayesian","_id":"cla4bteu20003chvg0bw8bqo5"},{"name":"JAGS","_id":"cla4bteu20004chvge28kbqkr"}]}}